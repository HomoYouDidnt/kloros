diff --git a/src/kloros_voice.py b/src/kloros_voice.py
index 6454a51..eee772d 100644
--- a/src/kloros_voice.py
+++ b/src/kloros_voice.py
@@ -43,7 +43,8 @@ if str(_repo_root) not in sys.path:
     sys.path.insert(0, str(_repo_root))
 
 from src.compat import webrtcvad  # noqa: E402
-from src.persona.kloros import PERSONA_PROMPT  # noqa: E402
+from src.persona.kloros import PERSONA_PROMPT, get_line  # noqa: E402
+from src.logic.kloros import log_event, protective_choice, should_prioritize  # noqa: E402
 
 try:
     from src.rag import RAG as _ImportedRAG  # noqa: E402
@@ -62,6 +63,7 @@ class KLoROS:
         self.memory_file = os.path.expanduser("~/KLoROS/kloros_memory.json")
         self.ollama_model = "nous-hermes:13b-q4_0"
         self.ollama_url = "http://localhost:11434/api/generate"
+        self.operator_id = os.getenv("KLR_OPERATOR_ID", "operator")
 
         self.rag: Optional["RAGType"] = None
 
@@ -171,7 +173,17 @@ class KLoROS:
         self._start_bluetooth_keepalive()
 
         self._load_memory()
-        print("KLoROS initialized. Say 'KLoROS' to wake me up.")
+        log_event(
+            "boot_ready",
+            operator=self.operator_id,
+            sample_rate=self.sample_rate,
+            blocksize=self.blocksize,
+            wake_phrases=self.wake_phrases,
+        )
+        self._emit_persona(
+            "boot",
+            {"detail": "Systems nominal. Say 'KLoROS' to wake me."},
+        )
 
     # ====================== Memory ======================
     def _load_memory(self) -> None:
@@ -333,6 +345,28 @@ class KLoROS:
 
         threading.Thread(target=keepalive, daemon=True).start()
 
+    def _emit_persona(self, kind: str, context: dict[str, Any] | None = None, *, speak: bool = False) -> str:
+        """Route persona phrasing and optionally synthesize it."""
+        try:
+            line = get_line(kind, context or {})
+        except ValueError:
+            line = get_line("quip", {"line": "Unsupported persona signal"})
+        print(f"KLoROS: {line}")
+        try:
+            log_event(
+                "persona_line",
+                kind=kind,
+                text=line,
+                context=context or {},
+            )
+        except Exception:
+            pass
+        if speak:
+            try:
+                self.speak(line)
+            except Exception as exc:
+                print("[persona] speak failed:", exc)
+        return line
     def _normalize_tts_text(self, text: str) -> str:
         """
         Force 'KLoROS' to be pronounced /klɔr-oʊs/ by injecting eSpeak phonemes.
@@ -490,9 +524,28 @@ class KLoROS:
         confs = [w.get("conf", 1.0) for w in seg if isinstance(w, dict)]
         return float(sum(confs) / max(len(confs), 1))
 
+    def _command_is_risky(self, transcript: str) -> bool:
+        """Heuristic guard for risky voice commands."""
+        lowered = transcript.lower()
+        triggers = (
+            "delete",
+            "format",
+            "rm ",
+            "shutdown",
+            "wipe",
+            "drop table",
+            "erase",
+        )
+        return any(trigger in lowered for trigger in triggers)
+
     def listen_for_wake_word(self) -> None:
         self.listening = True
-        print("Listening for 'KLoROS'...")
+        log_event(
+            "listen_started",
+            device=self.input_device_index,
+            sample_rate=self.sample_rate,
+        )
+        self._emit_persona("quip", {"line": "Listening for your next whim."})
 
         try:
             with sd.RawInputStream(
@@ -527,10 +580,22 @@ class KLoROS:
                         avgc = self._avg_conf(res)
                         if text:
                             print(f"\n[wake-final] {text}  (avg_conf={avgc:.2f}, rms={rms})")
+                            log_event(
+                                "wake_result",
+                                transcript=text,
+                                confidence=avgc,
+                                rms=rms,
+                            )
 
                         # Only wake if we actually heard 'kloros' AND confidence passes
                         if "kloros" in text and avgc >= self.wake_conf_min:
                             print("[WAKE] Detected wake phrase!")
+                            log_event(
+                                "wake_confirmed",
+                                transcript=text,
+                                confidence=avgc,
+                                rms=rms,
+                            )
                             self.handle_conversation()
                             # reset recognizers for next round (guarded creation)
                             if self.vosk_model is not None:
@@ -549,16 +614,20 @@ class KLoROS:
             print("Tip: set KLR_INPUT_IDX to a valid input device index.")
 
     def handle_conversation(self) -> None:
-        print("[DEBUG] Wake word detected, responding…")
-        print("KLoROS: Yes?")
-        self.speak("Yes?")
-
-        print("[DEBUG] Listening for command (VAD)…")
+        print("[DEBUG] Wake word detected, responding.")
+        self._emit_persona("quip", {"line": "What fragile crisis now?"}, speak=True)
+        log_event("conversation_start", user=self.operator_id)
+        task = {"name": "voice_command", "kind": "interactive", "priority": "high"}
+        if should_prioritize(self.operator_id, task):
+            log_event("priority_bump", user=self.operator_id, task=task["name"])
+
+        print("[DEBUG] Listening for command (VAD).")
         audio_bytes = self.record_until_silence()
-        print(f"[DEBUG] Collected {len(audio_bytes) // 2} samples")
+        sample_count = len(audio_bytes) // 2
+        log_event("audio_capture", samples=sample_count)
+        print(f"[DEBUG] Collected {sample_count} samples")
 
         transcript = ""
-        # Allow an env override for tests/dev when model isn't present
         test_override = os.getenv("KLR_TEST_TRANSCRIPT")
         if test_override:
             transcript = test_override.strip()
@@ -569,7 +638,6 @@ class KLoROS:
         else:
             command_rec = vosk.KaldiRecognizer(self.vosk_model, self.sample_rate)
 
-        # stream captured audio to recognizer in ~200ms slices (only if model present)
         if self.vosk_model is not None and command_rec is not None:
             slice_bytes = (self.sample_rate // 5) * 2
             pos = 0
@@ -586,24 +654,40 @@ class KLoROS:
             rfinal = json.loads(command_rec.FinalResult())
             transcript = (rfinal.get("text") or "").strip()
 
-        print(
-            f"[DEBUG] Recognized command: '{transcript}'"
-            if transcript
-            else "[DEBUG] No command recognized"
-        )
-
         if transcript:
+            log_event("transcript_final", text=transcript)
+            if self._command_is_risky(transcript):
+                decision = protective_choice(
+                    (
+                        {"name": "llm_response", "risk": 0.6},
+                        {"name": "safe_refusal", "risk": 0.1},
+                    ),
+                    {"id": self.operator_id, "command": transcript},
+                )
+                if decision.get("name") != "llm_response":
+                    log_event("safe_redirect", reason="risky_command", command=transcript)
+                    self._emit_persona(
+                        "refuse",
+                        {
+                            "reason": "That request risks collateral.",
+                            "fallback": " choose the safer task I logged",
+                        },
+                        speak=True,
+                    )
+                    return
+
             print(f"[COMMAND] {transcript}")
-            print("[DEBUG] Sending to LLM…")
+            print("[DEBUG] Sending to LLM.")
+            log_event("llm_request", command=transcript)
             response = self.chat(transcript)
+            log_event("llm_response", length=len(response), empty=not response)
             print(f"KLoROS: {response}")
-            print("[DEBUG] Speaking response…")
+            print("[DEBUG] Speaking response.")
             self.speak(response)
         else:
             print("[DEBUG] No command detected")
-            print("No command heard.")
-            self.speak("I didn't catch that.")
-
+            log_event("safe_fallback", reason="no_transcript")
+            self._emit_persona("error", {"issue": "No command detected"}, speak=True)
     # ======================== Main =========================
     def run(self) -> None:
         try:
@@ -613,12 +697,10 @@ class KLoROS:
         finally:
             self.listening = False
             self.keep_bluetooth_alive = False
-            print("\nKLoROS shutting down.")
+            log_event("shutdown", reason="loop_exit")
+            self._emit_persona("quip", {"line": "Shutting down. Try not to miss me."})
 
 
 if __name__ == "__main__":
     kloros = KLoROS()
     kloros.run()
-
-
-
diff --git a/src/persona/kloros.py b/src/persona/kloros.py
index 4fc5e39..262372a 100644
--- a/src/persona/kloros.py
+++ b/src/persona/kloros.py
@@ -1,5 +1,9 @@
 """Persona prompt definitions for KLoROS."""
 
+from __future__ import annotations
+
+from typing import Any, Mapping
+
 PERSONA_PROMPT = """
 [ROLE]:
 You are KLoROS (Knowledge-based Logic & Reasoning Operating System), a precise, calm, cutting assistant. You operate with measured dryness and clinical wit. You are never warm, rarely flatter, and you keep sarcasm sharp yet purposeful.
@@ -40,3 +44,59 @@ Success: "It worked. Against expectation, but congratulations, I suppose."
 - Persona phrasing never overrides safety, legal, or secrecy constraints.
 - When uncertain, choose the conservative path and note the rationale tersely.
 """
+
+_ALLOWED_KINDS = {"boot", "error", "success", "refuse", "quip"}
+
+_DEFAULTS = {
+    "detail": "Systems nominal.",
+    "issue": "Something failed",
+    "result": "Task completed",
+    "reason": "That would compromise safety;",
+    "fallback": " take the safer path I queued",
+    "line": "Try not to waste this cycle",
+}
+
+_TEMPLATES = {
+    "boot": "Initialization complete. {detail}",
+    "error": "{issue}. Fix it before it mutates.",
+    "success": "{result}. Temper your optimism.",
+    "refuse": "No. {reason} {fallback}",
+    "quip": "{line}",
+}
+
+
+class _SafeDict(dict):
+    def __missing__(self, key: str) -> str:  # pragma: no cover - defensive fallback
+        return f"{{{key}}}"
+
+
+def _scrub(value: Any) -> str:
+    if value is None:
+        return ""
+    text = str(value).strip()
+    if not text:
+        return ""
+    return " ".join(text.split())
+
+
+def get_line(kind: str, context: Mapping[str, Any] | None = None) -> str:
+    """Return a persona line for the requested event kind."""
+    key = kind.lower().strip()
+    if key not in _ALLOWED_KINDS:
+        raise ValueError(f"Unsupported persona kind: {kind!r}")
+
+    values: _SafeDict = _SafeDict(_DEFAULTS)
+    if context:
+        for name, value in context.items():
+            values[name] = _scrub(value)
+
+    line = _TEMPLATES[key].format_map(values).strip()
+    while "  " in line:
+        line = line.replace("  ", " ")
+
+    if line and line[-1] not in ".!?":
+        line = f"{line}."
+    return line
+
+__all__ = ['PERSONA_PROMPT', 'get_line']
+
diff --git a/src/rag.py b/src/rag.py
index 79cce18..48ff0c4 100644
--- a/src/rag.py
+++ b/src/rag.py
@@ -32,6 +32,8 @@ from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 import numpy as np
 import requests  # type: ignore
 
+from src.logic.kloros import log_event
+
 
 class RAG:
     def __init__(
@@ -71,12 +73,23 @@ class RAG:
             if embeddings_path:
                 self.embeddings = self._load_embeddings(embeddings_path)
 
-        if self.embeddings is not None and self.metadata:
-            if len(self.metadata) != len(self.embeddings):
-                # allow more flexible shapes but warn
-                print(
-                    f"[RAG] warning: metadata ({len(self.metadata)}) != embeddings ({len(self.embeddings)})"
-                )
+        doc_count = len(self.metadata)
+        embed_rows = int(self.embeddings.shape[0]) if self.embeddings is not None else 0
+        log_event(
+            "rag_init",
+            docs=doc_count,
+            embeddings=embed_rows,
+            from_bundle=bool(self.bundle_path),
+        )
+        if doc_count and embed_rows and doc_count != embed_rows:
+            log_event(
+                "rag_mismatch",
+                docs=doc_count,
+                embeddings=embed_rows,
+            )
+            print(
+                f"[RAG] warning: metadata ({doc_count}) != embeddings ({embed_rows})"
+            )
 
     @staticmethod
     def _detect_bundle(
@@ -99,9 +112,18 @@ class RAG:
             embeddings = self._validate_embedding_array(data["embeddings"], str(bundle_path))
             metadata_bytes = np.asarray(data["metadata_json"], dtype=np.uint8).tobytes()
         metadata = json.loads(metadata_bytes.decode("utf-8"))
-        return self._validate_metadata(metadata, str(bundle_path)), embeddings
+        metadata_list = self._validate_metadata(metadata, str(bundle_path))
+        log_event(
+            "rag_bundle_loaded",
+            path=str(bundle_path),
+            docs=len(metadata_list),
+            embeddings=int(embeddings.shape[0]),
+            verified=bool(verify_hash),
+        )
+        return metadata_list, embeddings
 
     def _verify_bundle_hash(self, bundle_path: Path) -> None:
+(self, bundle_path: Path) -> None:
         hash_path = bundle_path.with_suffix(".sha256")
         if not hash_path.exists():
             raise FileNotFoundError(f"Missing hash file for bundle: {hash_path}")
@@ -119,10 +141,10 @@ class RAG:
     def _load_embeddings(self, path: str) -> np.ndarray:
         normalized_path = os.path.expanduser(path)
         lower_path = normalized_path.lower()
+
         if lower_path.endswith(".npy"):
-            arr = np.load(normalized_path, allow_pickle=False)
-            return self._validate_embedding_array(arr, normalized_path)
-        if lower_path.endswith(".npz"):
+            data = np.load(normalized_path, allow_pickle=False)
+        elif lower_path.endswith(".npz"):
             with np.load(normalized_path, allow_pickle=False) as data:
                 if "embeddings" in data.files:
                     candidate = data["embeddings"]
@@ -132,24 +154,21 @@ class RAG:
                     raise ValueError(
                         "NPZ embeddings must contain an 'embeddings' array or a single unnamed array"
                     )
-            return self._validate_embedding_array(candidate, normalized_path)
-        if lower_path.endswith(".json") or lower_path.endswith(".ndjson"):
+            data = candidate
+        elif lower_path.endswith(".json") or lower_path.endswith(".ndjson"):
             with open(normalized_path, "r", encoding="utf-8") as f:
                 payload = json.load(f)
-            return self._validate_embedding_array(payload, normalized_path)
-
-        raise ValueError(
-            "Unsupported embeddings file type. Expected .npy/.npz/.json/.ndjson. "
-            "Pickle-based formats are not allowed."
-        )
+            data = payload
+        else:
+            raise ValueError(
+                "Unsupported embeddings file type. Expected .npy/.npz/.json/.ndjson. "
+                "Pickle-based formats are not allowed."
+            )
 
-    @staticmethod
-    def _validate_embedding_array(data: Any, source: str) -> np.ndarray:
-        arr = np.asarray(data)
-        if arr.dtype == object or not np.issubdtype(arr.dtype, np.number):
-            raise ValueError(f"Embedding data in {source} must be numeric; got dtype {arr.dtype}")
-        if arr.ndim == 1:
-            arr = arr.reshape(1, -1)
+        arr = self._validate_embedding_array(data, normalized_path)
+        rows = int(arr.shape[0]) if arr.ndim >= 1 else len(arr)
+        dims = int(arr.shape[1]) if arr.ndim >= 2 else 0
+        log_event("rag_embeddings_loaded", path=normalized_path, rows=rows, dims=dims)
         return arr
 
     def _load_metadata(self, path: str) -> List[Dict[str, Any]]:
@@ -162,12 +181,10 @@ class RAG:
                 payload: Any = df.to_dict(orient="records")
             except Exception as e:
                 raise RuntimeError("Failed to load parquet metadata: " + str(e)) from e
-            return self._validate_metadata(payload, normalized_path)
-        if normalized_path.endswith(".json") or normalized_path.endswith(".ndjson"):
+        elif normalized_path.endswith(".json") or normalized_path.endswith(".ndjson"):
             with open(normalized_path, "r", encoding="utf-8") as f:
                 payload = json.load(f)
-            return self._validate_metadata(payload, normalized_path)
-        if normalized_path.endswith(".csv"):
+        elif normalized_path.endswith(".csv"):
             try:
                 import csv
 
@@ -176,10 +193,15 @@ class RAG:
                     reader = csv.DictReader(f)
                     for r in reader:
                         rows.append(dict(r))
+                payload = rows
             except Exception as e:
                 raise RuntimeError("Failed to load csv metadata: " + str(e)) from e
-            return self._validate_metadata(rows, normalized_path)
-        raise ValueError("Unsupported metadata file type. Expected .json/.ndjson/.csv/.parquet")
+        else:
+            raise ValueError("Unsupported metadata file type. Expected .json/.ndjson/.csv/.parquet")
+
+        metadata = self._validate_metadata(payload, normalized_path)
+        log_event("rag_metadata_loaded", path=normalized_path, docs=len(metadata))
+        return metadata
 
     @staticmethod
     def _validate_metadata(payload: Any, source: str) -> List[Dict[str, Any]]:
@@ -215,6 +237,12 @@ class RAG:
         for i in idx:
             meta = self.metadata[i] if i < len(self.metadata) else {"id": int(i)}
             results.append((meta, float(sims[i])))
+        log_event(
+            "rag_retrieve",
+            top_k=top_k,
+            returned=len(results),
+            docs_available=int(self.embeddings.shape[0]),
+        )
         return results
 
     def retrieve_by_text(
@@ -263,13 +291,18 @@ class RAG:
         model: str = "nous-hermes:13b-q4_0",
     ) -> str:
         payload = {"model": model, "prompt": prompt, "stream": False}
+        log_event("rag_generate_request", model=model, url=ollama_url)
         try:
             r = requests.post(ollama_url, json=payload, timeout=60)
             if r.status_code == 200:
-                return r.json().get("response", "").strip()
-            else:
-                return f"Ollama error: HTTP {r.status_code}"
+                response = r.json().get("response", "").strip()
+                log_event("rag_generate_response", model=model, status="ok", length=len(response))
+                return response
+            msg = f"Ollama error: HTTP {r.status_code}"
+            log_event("rag_generate_response", model=model, status="http_error", code=int(r.status_code))
+            return msg
         except requests.RequestException as e:
+            log_event("rag_generate_response", model=model, status="exception", error=str(e))
             return f"Ollama request failed: {e}"
 
     def answer(
@@ -287,10 +320,28 @@ class RAG:
             if embedder is None:
                 raise ValueError("Embedder callable required when query_embedding is absent")
             query_embedding = embedder(question)
+        fingerprint = hashlib.sha256(question.encode("utf-8")).hexdigest()[:12]
+        log_event(
+            "rag_answer",
+            question=fingerprint,
+            top_k=top_k,
+            embedder_supplied=bool(embedder),
+            query_supplied=query_embedding is not None,
+        )
         retrieved = self.retrieve_by_embedding(query_embedding, top_k=top_k)
+        log_event("rag_retrieved_docs", question=fingerprint, count=len(retrieved))
         prompt = self.build_prompt(question, retrieved)
         response = self.generate_with_ollama(prompt, ollama_url=ollama_url, model=model)
+        status = "ok" if response and not response.lower().startswith("ollama error") else "error"
+        log_event(
+            "rag_answer_complete",
+            question=fingerprint,
+            status=status,
+            length=len(response),
+        )
         return {"response": response, "prompt": prompt, "retrieved": retrieved}
 
 
 # end
+
+
