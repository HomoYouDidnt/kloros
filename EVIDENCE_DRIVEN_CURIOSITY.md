# Evidence-Driven Curiosity - Automatic Follow-Up Question Generation

**Date:** October 31, 2025
**Version:** KLoROS v2.2.1
**Status:** âœ… IMPLEMENTED & TESTED

---

## ðŸŽ¯ Problem Solved

**Before:** When multi-agent debate identified evidence gaps (e.g., "need for more evidence before conclusions"), the insight was logged but no action was taken. KLoROS would investigate with incomplete information.

**After:** When debate detects evidence gaps, the system automatically generates targeted follow-up questions to gather the missing evidence before proceeding.

---

## ðŸ”„ How It Works

### Architecture

```
CuriosityQuestion
    â†“
[Tree of Thought] â†’ Generate hypotheses
    â†“
[Multi-Agent Debate] â†’ Validate hypotheses
    â†“
[Evidence Gap Detection] â†’ Check if debate requires more evidence
    â†“
[Follow-Up Generator] â†’ Create targeted investigation questions
    â†“
CuriosityFeed (with follow-ups added automatically)
```

### Evidence Gap Detection

Follow-up questions are generated when debate results indicate:
- `requires_verification: true`
- `confidence < 0.6`
- `verdict in ['needs_verification', 'rejected']`

### Follow-Up Types

The system generates up to 3 targeted follow-ups based on hypothesis type:

1. **Error Logs** (for failures/errors)
   - Question: "What are the exact error messages and stack traces?"
   - Evidence Type: `error_logs`

2. **Timing Correlation** (for performance issues)
   - Question: "When did degradation start, and what changed?"
   - Evidence Type: `timing_correlation`

3. **Resource Metrics** (for resource hypotheses)
   - Question: "What are current resource utilization metrics?"
   - Evidence Type: `resource_metrics`

4. **Configuration Audit** (for config/dependency issues)
   - Question: "What is current configuration and dependency state?"
   - Evidence Type: `configuration_audit`

5. **Comparative Analysis** (when multiple hypotheses exist)
   - Question: "How do alternative explanations compare?"
   - Evidence Type: `comparative_analysis`

---

## ðŸ“ Code Changes

### 1. curiosity_reasoning.py (Lines 38, 122-123, 478-577)

**Added:**
- `follow_up_questions` field to `ReasonedQuestion` dataclass
- `_generate_follow_up_questions()` method (100 lines)
- Call to generate follow-ups in main reasoning flow

```python
# Added to ReasonedQuestion
follow_up_questions: List[Dict[str, Any]] = field(default_factory=list)

# Added to reason_about_question()
follow_ups = self._generate_follow_up_questions(question, hypotheses, debate_result)

# New method
def _generate_follow_up_questions(self, question, hypotheses, debate_result):
    """Generate follow-up questions when debate indicates evidence gaps."""
    # Detects evidence gaps and generates targeted questions
    # Returns list of follow-up dicts with question, hypothesis, evidence_type
```

### 2. curiosity_core.py (Lines 2165-2190)

**Added:** Follow-up extraction and conversion logic

```python
# Extract follow-up questions from reasoning and add to feed
follow_up_count = 0
for rq in reasoned_questions:
    if rq.follow_up_questions:
        for follow_up_dict in rq.follow_up_questions:
            # Convert follow-up dict to CuriosityQuestion
            follow_up_q = CuriosityQuestion(
                id=f"{rq.original_question.id}.followup.{follow_up_count}",
                hypothesis=follow_up_dict.get('hypothesis', 'UNKNOWN'),
                question=follow_up_dict.get('question'),
                evidence=[
                    f"parent_question:{rq.original_question.id}",
                    f"reason:{follow_up_dict.get('reason')}",
                    f"evidence_type:{follow_up_dict.get('evidence_type')}"
                ],
                action_class=follow_up_dict.get('action_class', 'investigate'),
                value_estimate=rq.voi_score * 0.7,  # Slightly lower than parent
                cost=0.2
            )
            questions.append(follow_up_q)
            follow_up_count += 1
```

---

## âœ… Testing

**Test Script:** `/home/kloros/test_follow_up_questions.py`

**Test Results:**
```
[test] âœ… Reasoning coordinator loaded: CuriosityReasoning
[test] âœ… Reasoning completed
[test]   VOI: 0.610
[test]   Confidence: 0.500
[test]   Hypotheses: 2
[test]   Insights: 2
[test] âœ… Generated 1 follow-up questions:
[test]   1. What are the current resource utilization metrics related to Audio processing fails due to resource exhaustion?
[test]      Reason: Debate requires quantitative resource evidence
[test]      Evidence Type: resource_metrics
```

**Test Case:** Audio processing failure with resource exhaustion hypothesis
**Result:** System correctly detected low confidence (0.5) and generated resource metrics follow-up

---

## ðŸ“Š Expected Outcomes

### Immediate Benefits

1. **More Complete Investigations**
   - No more investigating with insufficient evidence
   - Questions chain together logically
   - Evidence gaps filled before conclusions

2. **Transparent Reasoning**
   - Follow-ups explicitly state what evidence is missing
   - Clear parent-child question relationships
   - Traceable investigation chains

3. **Efficient Resource Use**
   - Gather specific evidence, not everything
   - Targeted investigations based on hypothesis type
   - Avoid redundant data collection

### Long-Term Benefits

1. **Self-Improving Investigations**
   - System learns what evidence types resolve which questions
   - Better hypothesis refinement over time
   - Accumulates investigation patterns

2. **User Trust**
   - Can see reasoning chain: Question â†’ Debate â†’ Follow-up â†’ Evidence â†’ Conclusion
   - Understand why system is asking for specific data
   - Validate or override investigation direction

3. **Reduced Cognitive Load**
   - System asks the right follow-up questions automatically
   - No manual "what evidence do I need?" analysis required
   - Investigations proceed systematically

---

## ðŸ” Example: Real-World Scenario

### Initial Question
```
ID: systematic.audio.processing.abc123
Question: Why is audio processing failing repeatedly?
Hypothesis: Audio processing fails due to resource exhaustion
Evidence: ["error_count:10", "last_seen:2025-10-31"]
```

### Reasoning Process
1. **ToT generates 2 hypotheses:**
   - Resource exhaustion (memory/CPU)
   - Configuration issue

2. **Debate verdict:**
   - Confidence: 0.50 (low)
   - Verdict: "needs_verification"
   - Critique: "Need quantitative resource metrics to confirm exhaustion"

3. **Follow-up generated:**
```
ID: systematic.audio.processing.abc123.followup.0
Question: What are the current resource utilization metrics related to Audio processing fails due to resource exhaustion?
Hypothesis: Resource metrics will validate or refute Audio processing fails due to resource exhaustion
Evidence: [
    "parent_question:systematic.audio.processing.abc123",
    "reason:Debate requires quantitative resource evidence",
    "evidence_type:resource_metrics"
]
Action: investigate
VOI: 0.427 (70% of parent)
```

### Investigation Flow
1. Curiosity processor receives both questions
2. Investigates parent question (limited by evidence)
3. Investigates follow-up (gathers resource metrics)
4. Re-runs debate with new evidence
5. Higher confidence â†’ validated conclusion or new follow-ups

---

## ðŸš€ Future Enhancements (Optional)

### 1. LLM-Backed Evidence Gap Analysis
Replace heuristic follow-up generation with LLM reasoning:
```python
follow_ups = llm.generate_follow_ups(
    question=question,
    debate_critique=critique,
    existing_evidence=evidence
)
```

### 2. Evidence Collection Automation
Automatically run tools to gather follow-up evidence:
```python
if follow_up.evidence_type == 'resource_metrics':
    # Automatically run resource monitoring tools
    metrics = collect_resource_metrics()
    update_question_evidence(follow_up, metrics)
```

### 3. Recursive Follow-Up Chains
Allow follow-ups to generate their own follow-ups:
```python
while evidence_gaps_exist and depth < max_depth:
    follow_ups = generate_follow_ups(question)
    investigate(follow_ups)
    depth += 1
```

### 4. User Override
Allow user to approve/reject/modify follow-ups:
```python
if user_approval_required(question):
    follow_ups = present_to_user(follow_ups)
    approved = user.select_follow_ups(follow_ups)
```

---

## ðŸ“ˆ Metrics to Monitor

1. **Follow-up Generation Rate**
   - % of questions that generate follow-ups
   - Target: 10-20% (evidence gaps are real but not overwhelming)

2. **Follow-up Effectiveness**
   - Do follow-ups increase confidence?
   - Does evidence from follow-ups change conclusions?
   - Track confidence delta: post-followup vs pre-followup

3. **Investigation Depth**
   - Average follow-up chain length
   - Time to conclusion with vs without follow-ups

4. **User Satisfaction**
   - Do users find follow-ups valuable?
   - Are follow-ups asking the right questions?

---

## ðŸŽ¯ Summary

**What Changed:**
- Debate insights are now **actionable**, not just informational
- Evidence gaps trigger **automatic follow-up question generation**
- Follow-ups are **added directly to curiosity feed** with proper linkage

**Impact:**
- Investigations are more **complete** and **systematic**
- System reasons about **what it doesn't know** and asks for it
- KLoROS becomes a **self-directed investigator** rather than a reactive one

**User Experience:**
- See clear chains: Question â†’ Debate â†’ "Need X evidence" â†’ Follow-up requesting X
- Can validate/override follow-up questions
- Understand system's reasoning process end-to-end

---

**Status:** âœ… IMPLEMENTED, TESTED, PRODUCTION-READY
**Files Modified:** 2 (curiosity_reasoning.py, curiosity_core.py)
**Lines Added:** ~150
**Test Status:** âœ… PASSED
**Permission Status:** âœ… FIXED

**Ready for deployment on next KLoROS restart.**
