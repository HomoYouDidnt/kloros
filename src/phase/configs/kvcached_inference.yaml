# PHASE Config: Unified Ollama Inference
# Single Ollama service loads models on-demand

inference:
  judge:
    backend: ollama
    base_url: http://127.0.0.1:11434/v1
    model: qwen2.5:7b-instruct-q4_K_M
    max_tokens: 1024  # Hard cap for KV budget
    temperature: 0.0  # Deterministic judging
    pool:
      max_concurrent: 2  # Conservative for bursty workload
      qps: 4.0  # Max queries per second
      timeout_sec: 60.0

  performer:
    backend: ollama
    # Existing Ollama configuration (unchanged)
    # Runs on GPU 1 (GTX 1080 Ti)

telemetry:
  kvcached:
    enabled: true
    log_path: /home/kloros/logs/inference
    capture:
      - latency_ms
      - tokens_in
      - tokens_out
      - tokens_total
      - kv_bytes        # Estimated from model specs
      - kv_evictions    # From vLLM logs if available
      - prefix_cache_hits  # Always 0 with KVCached (disabled)

gpu_topology:
  judge_gpu: 0  # RTX 3060 - vLLM+KVCached
  performer_gpu: 1  # GTX 1080 Ti - Ollama
  stt_gpu: 1  # GTX 1080 Ti - Shared with performer

# Success gates
gates:
  judge:
    min_availability: 0.99  # 99% uptime
    max_p95_latency_ms: 5000  # 5s p95 acceptable for judge
    max_oom_rate: 0.01  # <1% OOM events

  stt:
    max_p95_regression_pct: 10  # STT latency shouldn't degrade >10%

# Monitoring
watchdog:
  enabled: true
  interval_sec: 2
  log_to_journal: true
