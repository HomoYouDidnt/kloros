"""
SPICA Domain - Generic Test Domain for PHASE Tournaments

Provides generic test execution infrastructure for SPICA instances.
This is the base domain that tournament evaluators use.

Auto-generated by ModuleGenerator from existing spica_* patterns.
"""
import sys
import json
import logging
import subprocess
import time
from pathlib import Path
from typing import Dict, Any, List, Optional, NamedTuple
from dataclasses import dataclass, asdict

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = logging.getLogger(__name__)


class PytestResult(NamedTuple):
    """Result from pytest execution."""
    exit_code: int
    stdout: str
    stderr: str
    duration_ms: float
    tests_passed: int
    tests_failed: int
    tests_total: int


@dataclass
class SPICATestConfig:
    """Configuration for SPICA test execution."""
    max_latency_ms: int = 5000
    max_memory_mb: int = 2048
    max_cpu_percent: float = 80.0
    timeout_seconds: int = 120


@dataclass
class SPICATestResult:
    """Result from a SPICA test run."""
    replica_id: str
    spica_id: str
    status: str  # "pass", "fail", "timeout", "oom"
    exact_match_mean: float = 0.0
    latency_p50_ms: float = 0.0
    latency_p95_ms: float = 0.0
    memory_peak_mb: float = 0.0
    cpu_percent: float = 0.0
    query_count: int = 0
    error_message: Optional[str] = None


class SPICADomain:
    """
    Generic PHASE domain for SPICA instance testing.

    Provides infrastructure for:
    - Running QTIME-accelerated test replicas
    - Aggregating results across replicas
    - Enforcing resource limits and timeouts
    """

    def __init__(self, config: SPICATestConfig):
        """
        Initialize SPICA test domain.

        Args:
            config: SPICATestConfig with test parameters
        """
        self.config = config
        self.results: List[SPICATestResult] = []

    def _execute_pytest_suite(
        self,
        instance_path: Path,
        timeout_seconds: int
    ) -> PytestResult:
        """
        Execute pytest test suite from SPICA instance.

        Args:
            instance_path: Path to SPICA instance directory
            timeout_seconds: Maximum execution time

        Returns:
            PytestResult with test outcomes and metrics
        """
        test_dir = instance_path / "tests"

        if not test_dir.exists():
            logger.warning(f"No tests directory found in {instance_path}")
            return PytestResult(
                exit_code=5,
                stdout="",
                stderr="No tests directory found",
                duration_ms=0.0,
                tests_passed=0,
                tests_failed=0,
                tests_total=0
            )

        start_time = time.time()

        try:
            template_venv_python = Path("/home/kloros/experiments/spica/template/.venv/bin/python")

            if not template_venv_python.exists():
                logger.error(f"Template venv not found: {template_venv_python}")
                return PytestResult(
                    exit_code=5,
                    stdout="",
                    stderr=f"Template venv missing: {template_venv_python}",
                    duration_ms=0.0,
                    tests_passed=0,
                    tests_failed=0,
                    tests_total=0
                )

            cmd = [
                str(template_venv_python),
                "-m",
                "pytest",
                str(test_dir),
                "-v",
                "--tb=short",
                "-x",
                "-o", "addopts="  # Override pytest.ini to avoid requiring missing plugins
            ]

            result = subprocess.run(
                cmd,
                cwd=str(instance_path),
                capture_output=True,
                text=True,
                timeout=timeout_seconds + 10
            )

            duration_ms = (time.time() - start_time) * 1000

            logger.info(f"[PYTEST_DEBUG] stdout (last 500 chars): {result.stdout[-500:]}")
            logger.info(f"[PYTEST_DEBUG] return code: {result.returncode}")
            passed, failed, total = self._parse_pytest_summary(result.stdout)
            logger.info(f"[PYTEST_DEBUG] Parsed counts: passed={passed}, failed={failed}, total={total}")

            return PytestResult(
                exit_code=result.returncode,
                stdout=result.stdout,
                stderr=result.stderr,
                duration_ms=duration_ms,
                tests_passed=passed,
                tests_failed=failed,
                tests_total=total
            )

        except subprocess.TimeoutExpired as e:
            duration_ms = (time.time() - start_time) * 1000
            return PytestResult(
                exit_code=124,
                stdout=e.stdout.decode() if e.stdout else "",
                stderr=f"Test timeout after {timeout_seconds}s",
                duration_ms=duration_ms,
                tests_passed=0,
                tests_failed=0,
                tests_total=0
            )

        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000
            return PytestResult(
                exit_code=1,
                stdout="",
                stderr=str(e),
                duration_ms=duration_ms,
                tests_passed=0,
                tests_failed=0,
                tests_total=0
            )

    def _parse_pytest_summary(self, stdout: str) -> tuple[int, int, int]:
        """
        Parse pytest output to extract test counts.

        Args:
            stdout: pytest stdout output

        Returns:
            Tuple of (passed, failed, total)
        """
        import re

        passed = 0
        failed = 0

        for line in stdout.split('\n'):
            if ' passed' in line:
                match = re.search(r'(\d+)\s+passed', line)
                if match:
                    passed = int(match.group(1))
            if ' failed' in line:
                match = re.search(r'(\d+)\s+failed', line)
                if match:
                    failed = int(match.group(1))

        total = passed + failed
        return passed, failed, total

    def run_qtime_replicas(
        self,
        instance_paths: List[Path],
        replica_plan: Dict[str, Any]
    ) -> List[SPICATestResult]:
        """
        Run QTIME-accelerated replicas for SPICA instances.

        Args:
            instance_paths: List of SPICA instance directories
            replica_plan: Deterministic replica expansion plan

        Returns:
            List of SPICATestResult objects
        """
        results = []

        for replica in replica_plan["replicas"]:
            replica_id = replica["replica_id"]
            instance_id = replica["instance_id"]

            # Find instance path
            instance_path = None
            for path in instance_paths:
                if instance_id in str(path):
                    instance_path = path
                    break

            if not instance_path:
                logger.warning(f"Instance not found for {instance_id}")
                continue

            # Run test for this replica
            result = self._run_single_replica(
                replica_id=replica_id,
                instance_id=instance_id,
                instance_path=instance_path,
                epoch=replica["epoch"],
                slice_idx=replica["slice"],
                replica_idx=replica["replica"]
            )

            results.append(result)

        self.results = results
        return results

    def _run_single_replica(
        self,
        replica_id: str,
        instance_id: str,
        instance_path: Path,
        epoch: int,
        slice_idx: int,
        replica_idx: int
    ) -> SPICATestResult:
        """
        Run a single test replica by executing the SPICA instance's pytest suite.

        Args:
            replica_id: Unique replica identifier
            instance_id: SPICA instance ID
            instance_path: Path to SPICA instance directory
            epoch: Epoch number in QTIME plan
            slice_idx: Slice index in QTIME plan
            replica_idx: Replica index in QTIME plan

        Returns:
            SPICATestResult with actual test outcomes and metrics
        """
        manifest_path = instance_path / "manifest.json"

        if not manifest_path.exists():
            return SPICATestResult(
                replica_id=replica_id,
                spica_id=instance_id,
                status="fail",
                error_message=f"Manifest not found: {manifest_path}"
            )

        try:
            manifest = json.loads(manifest_path.read_text())
            logger.info(f"Running tests for {replica_id} (instance: {instance_id})")

            pytest_result = self._execute_pytest_suite(
                instance_path=instance_path,
                timeout_seconds=self.config.timeout_seconds
            )

            if pytest_result.exit_code == 124:
                status = "timeout"
            elif pytest_result.exit_code == 0:
                status = "pass"
            elif pytest_result.exit_code == 5:
                status = "fail"
                pytest_result = pytest_result._replace(
                    stderr="No tests collected"
                )
            else:
                status = "fail"

            pass_rate = (
                pytest_result.tests_passed / pytest_result.tests_total
                if pytest_result.tests_total > 0
                else 0.0
            )

            return SPICATestResult(
                replica_id=replica_id,
                spica_id=instance_id,
                status=status,
                exact_match_mean=pass_rate,
                latency_p50_ms=pytest_result.duration_ms,
                latency_p95_ms=pytest_result.duration_ms * 1.2,
                memory_peak_mb=0.0,
                cpu_percent=0.0,
                query_count=pytest_result.tests_total,
                error_message=pytest_result.stderr if status != "pass" else None
            )

        except Exception as e:
            logger.error(f"Test execution failed for {replica_id}: {e}")
            return SPICATestResult(
                replica_id=replica_id,
                spica_id=instance_id,
                status="fail",
                error_message=str(e)
            )

    def aggregate_replica_results(
        self,
        results: List[SPICATestResult]
    ) -> Dict[str, Any]:
        """
        Aggregate results across replicas by instance.

        Args:
            results: List of SPICATestResult objects

        Returns:
            Dict mapping instance_id to aggregated metrics
        """
        aggregated = {}

        # Group by SPICA instance
        by_instance = {}
        for result in results:
            if result.spica_id not in by_instance:
                by_instance[result.spica_id] = []
            by_instance[result.spica_id].append(result)

        # Aggregate metrics
        for instance_id, instance_results in by_instance.items():
            passed = sum(1 for r in instance_results if r.status == "pass")
            total = len(instance_results)

            # Average metrics across passing replicas
            passing = [r for r in instance_results if r.status == "pass"]
            if passing:
                avg_latency = sum(r.latency_p50_ms for r in passing) / len(passing)
                avg_exact_match = sum(r.exact_match_mean for r in passing) / len(passing)
            else:
                avg_latency = 0.0
                avg_exact_match = 0.0

            aggregated[instance_id] = {
                "pass_rate": passed / total if total > 0 else 0.0,
                "total_replicas": total,
                "passed": passed,
                "failed": total - passed,
                "avg_latency_p50_ms": avg_latency,
                "avg_exact_match_mean": avg_exact_match
            }

        return aggregated


# Export main classes
__all__ = ["SPICADomain", "SPICATestConfig", "SPICATestResult"]
