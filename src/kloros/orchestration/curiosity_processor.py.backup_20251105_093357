#!/usr/bin/env python3
"""
Curiosity Processor - Converts CuriosityCore questions into actionable intents.

Monitors curiosity_feed.json and spawns D-REAM experiments for high-value questions.
PHASE is for validation, not triggering - curiosity drives exploration immediately.
"""

import json
import hashlib
import logging
import subprocess
import os
import time
import contextlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

try:
    import filelock
except ImportError:
    filelock = None

logger = logging.getLogger(__name__)

CURIOSITY_FEED = Path("/home/kloros/.kloros/curiosity_feed.json")
SELF_STATE = Path("/home/kloros/.kloros/self_state.json")
INTENT_DIR = Path("/home/kloros/.kloros/intents")
PROCESSED_QUESTIONS = Path("/home/kloros/.kloros/processed_questions.jsonl")
LOCKS_DIR = Path("/home/kloros/.kloros/locks")
JOURNAL_DIR = Path("/home/kloros/.kloros/journals")
SPICA_INSTANCES_DIR = Path("/home/kloros/.kloros/spica/instances")

# Value/cost ratio threshold for spawning D-REAM experiments
VALUE_THRESHOLD = 1.5  # Questions with ratio > 1.5 trigger experiments

def _question_to_intent(question: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert a CuriosityQuestion to an intent for D-REAM.

    Args:
        question: CuriosityQuestion dict from curiosity_feed.json

    Returns:
        Intent dict for orchestrator consumption
    """
    qid = question["id"]
    hypothesis = question["hypothesis"]
    q_text = question["question"]
    action_class = question["action_class"]
    value = question["value_estimate"]
    cost = question["cost"]
    capability_key = question.get("capability_key", "unknown")

    # Map action classes to intent types
    if action_class == "propose_fix":
        intent_type = "curiosity_propose_fix"
        priority = 7
    elif action_class == "investigate":
        intent_type = "curiosity_investigate"
        priority = 6
    elif action_class == "find_substitute":
        intent_type = "curiosity_find_substitute"
        priority = 5
    else:
        intent_type = "curiosity_explore"
        priority = 4

    # Build D-REAM experiment suggestion
    experiment_hint = {
        "hypothesis": hypothesis,
        "search_space": _derive_search_space(question),
        "fitness_metric": _derive_fitness_metric(question),
        "exploration_budget": _derive_budget(value, cost)
    }

    intent = {
        "intent_type": intent_type,
        "priority": priority,
        "reason": f"CuriosityCore question: {hypothesis}",
        "data": {
            "question_id": qid,
            "question": q_text,
            "hypothesis": hypothesis,
            "capability_key": capability_key,
            "value_estimate": value,
            "cost_estimate": cost,
            "action_class": action_class,
            "evidence": question.get("evidence", []),
            "dream_experiment": experiment_hint
        },
        "generated_at": datetime.now().timestamp(),
        "emitted_by": "curiosity_processor"
    }

    return intent


def _derive_search_space(question: Dict[str, Any]) -> Dict[str, Any]:
    """
    Derive D-REAM search space from question context.

    Examples:
    - Swap pressure → memory management strategies
    - OOM errors → GPU allocation parameters
    - Missing capability → installation/substitution strategies
    """
    hypothesis = question["hypothesis"].lower()
    evidence = question.get("evidence", [])

    # Parse evidence for domain-specific hints
    domain = "unknown"
    for ev in evidence:
        if ev.startswith("capability:"):
            domain = ev.split(":", 1)[1]
            break

    if "swap" in hypothesis or "memory" in hypothesis:
        return {
            "type": "memory_management",
            "parameters": {
                "restart_strategy": ["periodic", "threshold", "adaptive"],
                "memory_limit_mb": [2048, 4096, 8192, 12288],
                "oom_score_adj": [-1000, -500, 0, 500]
            },
            "domain": domain
        }
    elif "oom" in hypothesis or "gpu" in hypothesis:
        return {
            "type": "gpu_allocation",
            "parameters": {
                "tensor_parallel_size": [1, 2],
                "gpu_memory_utilization": [0.70, 0.75, 0.80, 0.85, 0.90],
                "max_num_seqs": [32, 64, 128, 256]
            },
            "domain": domain
        }
    elif "missing" in hypothesis or "installation" in hypothesis:
        return {
            "type": "capability_setup",
            "parameters": {
                "install_method": ["pip", "apt", "manual", "substitute"],
                "dependency_strategy": ["full", "minimal", "optional"]
            },
            "domain": domain
        }
    else:
        return {
            "type": "generic_exploration",
            "parameters": {},
            "domain": domain
        }


def _derive_fitness_metric(question: Dict[str, Any]) -> str:
    """Derive appropriate fitness metric for D-REAM experiment."""
    hypothesis = question["hypothesis"].lower()

    if "swap" in hypothesis or "memory" in hypothesis:
        return "swap_usage_reduction"
    elif "oom" in hypothesis:
        return "oom_prevention_rate"
    elif "latency" in hypothesis:
        return "latency_p50_improvement"
    elif "pass_rate" in hypothesis or "accuracy" in hypothesis:
        return "test_pass_rate"
    else:
        return "capability_availability"


def _derive_budget(value: float, cost: float) -> Dict[str, int]:
    """
    Derive exploration budget based on value/cost ratio.

    Higher value questions get more D-REAM resources.
    """
    ratio = value / max(cost, 0.01)

    if ratio >= 3.0:
        # Critical - large exploration
        return {"max_trials": 50, "max_time_minutes": 30}
    elif ratio >= 2.0:
        # High value - moderate exploration
        return {"max_trials": 30, "max_time_minutes": 20}
    elif ratio >= 1.5:
        # Medium value - quick exploration
        return {"max_trials": 15, "max_time_minutes": 10}
    else:
        # Low value - minimal exploration
        return {"max_trials": 5, "max_time_minutes": 5}


def _is_question_processed(qid: str) -> bool:
    """Check if question has already been processed."""
    if not PROCESSED_QUESTIONS.exists():
        return False

    try:
        with open(PROCESSED_QUESTIONS, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                entry = json.loads(line)
                if entry.get("question_id") == qid:
                    return True
    except Exception as e:
        logger.warning(f"Error reading processed questions: {e}")

    return False


def _mark_question_processed(qid: str, intent_sha: str):
    """Mark question as processed."""
    PROCESSED_QUESTIONS.parent.mkdir(parents=True, exist_ok=True)

    entry = {
        "question_id": qid,
        "processed_at": datetime.now().timestamp(),
        "intent_sha": intent_sha
    }

    with open(PROCESSED_QUESTIONS, 'a') as f:
        f.write(json.dumps(entry) + '\n')


def _has_spawned_curiosity(qid: str) -> bool:
    """
    Check if we've already spawned experiments for this question.

    Makes re-runs idempotent by checking both intents and SPICA instances.
    """
    intents = list(INTENT_DIR.glob(f"curiosity_*{qid}.json"))
    archived_intents = list((INTENT_DIR / "processed").glob(f"**/curiosity_*{qid}.json"))

    if SPICA_INSTANCES_DIR.exists():
        spica = list(SPICA_INSTANCES_DIR.glob(f"*{qid}*"))
    else:
        spica = []

    return bool(intents or archived_intents or spica)


def _processed_older_than(qid: str, days: int) -> bool:
    """
    Check if question was processed more than N days ago.

    Returns True if question is old enough to allow re-processing.
    """
    if not PROCESSED_QUESTIONS.exists():
        return True

    cutoff = time.time() - (days * 86400)

    try:
        with open(PROCESSED_QUESTIONS, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                entry = json.loads(line)
                if entry.get("question_id") == qid:
                    processed_at = entry.get("processed_at", 0)
                    return processed_at < cutoff
    except Exception as e:
        logger.warning(f"Error checking processed age for {qid}: {e}")

    return True


def _reprocess_window_allows(qid: str) -> bool:
    """
    Check if re-processing window allows this question to be processed again.

    Uses KLR_CURIOSITY_REPROCESS_DAYS environment variable (default: 7 days).
    """
    days = int(os.environ.get("KLR_CURIOSITY_REPROCESS_DAYS", "7"))
    return _processed_older_than(qid, days)


@contextlib.contextmanager
def _curiosity_txn(qid: str):
    """
    Create a transaction context with file lock and journal for crash safety.

    The lock prevents concurrent processing of the same question.
    The journal enables crash recovery to detect partial processing.
    """
    LOCKS_DIR.mkdir(parents=True, exist_ok=True)
    JOURNAL_DIR.mkdir(parents=True, exist_ok=True)

    lock_path = LOCKS_DIR / f"curiosity_{qid}.lock"
    journal_path = JOURNAL_DIR / f"curiosity_{qid}.journal"

    if filelock is None:
        logger.warning(f"filelock not available, proceeding without lock for {qid}")
        try:
            journal_path.write_text(json.dumps({"stage": "begin", "qid": qid, "ts": time.time()}))
            yield {"journal": journal_path}
            journal_path.write_text(json.dumps({"stage": "commit", "qid": qid, "ts": time.time()}))
        finally:
            try:
                journal_path.unlink()
            except:
                pass
    else:
        with filelock.FileLock(str(lock_path), timeout=30):
            try:
                journal_path.write_text(json.dumps({"stage": "begin", "qid": qid, "ts": time.time()}))
                yield {"journal": journal_path}
                journal_path.write_text(json.dumps({"stage": "commit", "qid": qid, "ts": time.time()}))
            finally:
                try:
                    journal_path.unlink()
                except:
                    pass


def _cleanup_stale_processed_questions(max_age_days: int = None, max_entries: int = None):
    """
    Archive processed questions older than max_age_days and enforce max size with LRU eviction.

    This prevents the processed_questions.jsonl from growing indefinitely
    and allows re-processing of questions after they become stale.

    Args:
        max_age_days: Age threshold for archiving (default from KLR_CURIOSITY_REPROCESS_DAYS or 7 days)
        max_entries: Maximum entries to keep (default from KLR_CURIOSITY_MAX_PROCESSED or 500)
    """
    if not PROCESSED_QUESTIONS.exists():
        return

    if max_age_days is None:
        max_age_days = int(os.environ.get("KLR_CURIOSITY_REPROCESS_DAYS", "7"))
    if max_entries is None:
        max_entries = int(os.environ.get("KLR_CURIOSITY_MAX_PROCESSED", "500"))

    try:
        cutoff_time = datetime.now().timestamp() - (max_age_days * 86400)
        fresh_entries = []
        archived_count = 0

        # Read all entries and filter out stale ones
        with open(PROCESSED_QUESTIONS, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                entry = json.loads(line)
                processed_at = entry.get("processed_at", 0)

                if processed_at > cutoff_time:
                    fresh_entries.append(entry)
                else:
                    archived_count += 1

        # LRU eviction: keep only most recent max_entries
        if len(fresh_entries) > max_entries:
            # Sort by processed_at (oldest first)
            fresh_entries.sort(key=lambda e: e.get("processed_at", 0))
            # Keep only the newest max_entries
            evicted = len(fresh_entries) - max_entries
            fresh_entries = fresh_entries[-max_entries:]
            logger.info(f"LRU eviction: removed {evicted} oldest entries (kept {max_entries})")

        # Rewrite file with only fresh entries
        if archived_count > 0 or len(fresh_entries) != (archived_count + len(fresh_entries)):
            with open(PROCESSED_QUESTIONS, 'w') as f:
                for entry in fresh_entries:
                    f.write(json.dumps(entry) + '\n')

            logger.info(f"Archived {archived_count} stale processed questions (>{max_age_days} days old), "
                       f"kept {len(fresh_entries)} fresh entries")

    except Exception as e:
        logger.error(f"Error cleaning up processed questions: {e}")


def _spawn_direct_experiment(question: Dict[str, Any]) -> Dict[str, Any]:
    """
    Spawn a single D-REAM experiment for direct-build mode.

    Used when KLoROS provides specific guidance/hypothesis.
    """
    try:
        from src.dream.config_tuning.spica_spawner import spawn_instance

        # Create candidate from question hypothesis
        candidate = {
            "hypothesis": question["hypothesis"],
            "capability_key": question.get("capability_key", "unknown"),
            "value_estimate": question["value_estimate"]
        }

        instance = spawn_instance(
            candidate=candidate,
            parent_id=None,
            notes=f"Curiosity direct-build: {question['id']}"
        )

        logger.info(f"Spawned direct-build SPICA instance: {instance.spica_id}")
        return {
            "mode": "direct_build",
            "status": "spawned",
            "spica_id": instance.spica_id,
            "question_id": question["id"]
        }

    except Exception as e:
        logger.error(f"Failed to spawn direct experiment: {e}", exc_info=True)
        return {
            "mode": "direct_build",
            "status": "error",
            "error": str(e)
        }


def _spawn_tournament(question: Dict[str, Any]) -> Dict[str, Any]:
    """
    Spawn a D-REAM tournament for open exploration.

    Creates 8+ SPICA cells with different strategies, bracket elimination.
    """
    try:
        from src.dream.evaluators.spica_tournament_evaluator import SPICATournamentEvaluator

        # Prepare tournament context
        context = {
            "question_id": question["id"],
            "hypothesis": question["hypothesis"],
            "search_space": _derive_search_space(question),
            "advancement_metric": "speed"
        }

        evaluator = SPICATournamentEvaluator(
            suite_id=f"curiosity.{question.get('capability_key', 'unknown')}",
            qtime={"epochs": 1, "slices_per_epoch": 4, "replicas_per_slice": 2}
        )

        # Generate 8 candidate strategies (minimum bracket size)
        candidates = _generate_candidate_strategies(question, min_count=8)

        logger.info(f"Spawning tournament with {len(candidates)} candidates for {question['id']}")

        # Run bracket evaluation
        fitnesses, artifacts = evaluator.evaluate_batch(candidates, context)

        # Find champion
        champion_idx = fitnesses.index(max(fitnesses))
        champion = candidates[champion_idx]

        logger.info(f"Tournament complete: Champion fitness={fitnesses[champion_idx]:.3f}")

        return {
            "mode": "tournament",
            "status": "complete",
            "question_id": question["id"],
            "champion": champion,
            "champion_fitness": fitnesses[champion_idx],
            "total_candidates": len(candidates),
            "artifacts": artifacts
        }

    except Exception as e:
        logger.error(f"Failed to spawn tournament: {e}", exc_info=True)
        return {
            "mode": "tournament",
            "status": "error",
            "error": str(e)
        }


def _generate_candidate_strategies(question: Dict[str, Any], min_count: int = 8) -> List[Dict[str, Any]]:
    """Generate diverse candidate strategies for tournament."""
    hypothesis = question["hypothesis"].lower()
    search_space = _derive_search_space(question)

    # Base strategies that apply to most problems
    base_strategies = [
        {"name": "conservative", "temperature": 0.3, "explore": False},
        {"name": "aggressive", "temperature": 0.9, "explore": True},
        {"name": "balanced", "temperature": 0.6, "explore": True},
        {"name": "adaptive", "temperature": 0.7, "adaptive_temp": True},
    ]

    # Problem-specific strategies
    if "swap" in hypothesis or "memory" in hypothesis:
        strategies = base_strategies + [
            {"name": "periodic_restart", "restart_interval_hours": 4},
            {"name": "threshold_restart", "memory_threshold_gb": 8},
            {"name": "memory_limit", "max_memory_gb": 12},
            {"name": "oom_score_adjust", "oom_score": -500},
        ]
    elif "oom" in hypothesis or "gpu" in hypothesis:
        strategies = base_strategies + [
            {"name": "reduce_utilization", "gpu_memory_utilization": 0.75},
            {"name": "increase_utilization", "gpu_memory_utilization": 0.90},
            {"name": "reduce_batch_size", "max_num_seqs": 64},
            {"name": "tensor_parallel", "tensor_parallel_size": 2},
        ]
    else:
        # Generic exploration strategies
        strategies = base_strategies + [
            {"name": "fast_fallback", "timeout_ms": 500},
            {"name": "reliable_retry", "retry_count": 3},
            {"name": "cache_aggressive", "cache_size_mb": 1024},
            {"name": "lazy_load", "preload": False},
        ]

    # Ensure minimum count
    while len(strategies) < min_count:
        strategies.append({
            "name": f"variant_{len(strategies)}",
            "temperature": 0.5 + (len(strategies) * 0.05),
            "explore": len(strategies) % 2 == 0
        })

    return strategies[:max(min_count, len(strategies))]


def _check_for_stale_data() -> bool:
    """
    Check if curiosity feed is based on outdated capability state.

    Returns:
        True if feed should be regenerated, False otherwise
    """
    if not CURIOSITY_FEED.exists():
        return False  # No feed to be stale

    if not SELF_STATE.exists():
        return False  # No state to compare against

    try:
        state_mtime = SELF_STATE.stat().st_mtime
        feed_mtime = CURIOSITY_FEED.stat().st_mtime

        if state_mtime > feed_mtime:
            age_seconds = state_mtime - feed_mtime
            logger.warning(f"Stale curiosity feed detected: capability state updated {age_seconds:.1f}s after feed generation")
            return True

        return False
    except Exception as e:
        logger.error(f"Error checking for stale data: {e}")
        return False


def _regenerate_curiosity_feed() -> bool:
    """
    Regenerate curiosity feed from current capability state.

    Returns:
        True if regeneration succeeded, False otherwise
    """
    try:
        logger.info("Regenerating curiosity feed from updated capability state...")

        # Run curiosity_core module to regenerate feed
        result = subprocess.run(
            ["/home/kloros/.venv/bin/python3", "-m", "src.registry.curiosity_core"],
            cwd="/home/kloros",
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            logger.info("Successfully regenerated curiosity feed")
            return True
        else:
            logger.error(f"Failed to regenerate curiosity feed: {result.stderr[:200]}")
            return False

    except subprocess.TimeoutExpired:
        logger.error("Curiosity feed regeneration timed out after 30s")
        return False
    except Exception as e:
        logger.error(f"Error regenerating curiosity feed: {e}")
        return False


def process_curiosity_feed() -> Dict[str, Any]:
    """
    Process curiosity_feed.json and spawn D-REAM experiments for high-value questions.

    Returns:
        Dict with processing summary
    """
    # Check if curiosity processing is disabled
    if os.environ.get("KLR_DISABLE_CURIOSITY") == "1":
        return {"status": "disabled", "intents_emitted": 0, "experiments_spawned": 0}

    # Clean up stale processed questions to allow re-processing
    _cleanup_stale_processed_questions()

    # Check for stale data and regenerate if needed
    if _check_for_stale_data():
        logger.info("Detected stale curiosity feed - regenerating before processing")
        if not _regenerate_curiosity_feed():
            logger.warning("Feed regeneration failed, proceeding with existing feed")

    if not CURIOSITY_FEED.exists():
        logger.debug("No curiosity feed found")
        return {"status": "no_feed", "intents_emitted": 0, "experiments_spawned": 0}

    try:
        with open(CURIOSITY_FEED, 'r') as f:
            feed = json.load(f)
    except Exception as e:
        logger.error(f"Failed to read curiosity feed: {e}")
        return {"status": "error", "error": str(e), "intents_emitted": 0, "experiments_spawned": 0}

    questions = feed.get("questions", [])
    intents_emitted = 0
    experiments_spawned = 0
    skipped_low_value = 0
    skipped_processed = 0

    logger.info(f"Processing {len(questions)} curiosity questions")

    for q in questions:
        qid = q["id"]
        value = q["value_estimate"]
        cost = q["cost"]
        ratio = value / max(cost, 0.01)
        action_class = q["action_class"]
        hypothesis = q.get("hypothesis", "")

        # Check if already processed AND spawned (processed ≠ spawned)
        already_processed = _is_question_processed(qid)
        already_spawned = _has_spawned_curiosity(qid)
        reprocess_age_ok = _reprocess_window_allows(qid)

        # DIAGNOSTIC: Log decision variables for first 3 questions
        if skipped_processed + skipped_low_value < 3:
            logger.info(f"[DIAGNOSTIC] {qid}: processed={already_processed}, spawned={already_spawned}, age_ok={reprocess_age_ok}")

        # Skip only if processed AND (already spawned OR too recent to reprocess)
        if already_processed and (already_spawned or not reprocess_age_ok):
            skipped_processed += 1
            if skipped_processed <= 3:
                logger.info(f"[DIAGNOSTIC] Skipping {qid}: processed={already_processed} AND (spawned={already_spawned} OR age_not_ok={not reprocess_age_ok})")
            continue

        # Filter by value/cost ratio
        if ratio < VALUE_THRESHOLD:
            skipped_low_value += 1
            logger.debug(f"Skipping low-value question {qid} (ratio={ratio:.2f})")
            continue

        # INTEGRATION QUESTIONS: Route to BOTH documentation AND autonomous fix (if autonomy >= 3)
        if hypothesis.startswith(("ORPHANED_QUEUE_", "UNINITIALIZED_COMPONENT_", "DUPLICATE_")):
            try:
                from src.dream.remediation_manager import RemediationExperimentGenerator
                remediation = RemediationExperimentGenerator()
                fix_spec = remediation.generate_from_integration_question(q)

                if fix_spec:
                    autonomy = q.get("autonomy", 2)
                    logger.info(f"[integration_fix] Generated fix for {qid}: {fix_spec.get('action_type')} (autonomy={autonomy})")

                    # ALWAYS create documentation intent
                    doc_intent = {
                        "intent_type": "integration_fix",
                        "priority": 9,
                        "reason": f"Integration issue: {hypothesis}",
                        "data": {
                            "question_id": qid,
                            "question": q["question"],
                            "hypothesis": hypothesis,
                            "fix_specification": fix_spec,
                            "autonomy_level": autonomy
                        },
                        "generated_at": datetime.now().timestamp(),
                        "emitted_by": "curiosity_processor_integration_router"
                    }

                    doc_intent_json = json.dumps(doc_intent, indent=2)
                    doc_intent_sha = hashlib.sha256(doc_intent_json.encode()).hexdigest()[:16]
                    doc_intent_path = INTENT_DIR / f"integration_fix_{doc_intent_sha}_{qid}.json"
                    INTENT_DIR.mkdir(parents=True, exist_ok=True)
                    doc_intent_path.write_text(doc_intent_json)
                    intents_emitted += 1
                    logger.info(f"[integration_fix] Emitted documentation intent for {qid}")

                    # CONDITIONALLY create SPICA spawn intent for high autonomy
                    if autonomy >= 3:
                        evidence = q.get("evidence", [])

                        target_files = []
                        for ev in evidence:
                            if "Produced in:" in ev or "Found in:" in ev:
                                parts = ev.split(":")
                                if len(parts) >= 2:
                                    file_paths = parts[1].strip()
                                    for file_path in file_paths.split(","):
                                        file_path = file_path.strip()
                                        if file_path:
                                            target_files.append(file_path)

                        spica_intent = {
                            "intent_type": "spica_spawn_request",
                            "priority": q.get("priority", 8),
                            "reason": "Autonomous fix attempt for integration issue",
                            "data": {
                                "question_id": qid,
                                "question": q["question"],
                                "hypothesis": hypothesis,
                                "fix_context": {
                                    "evidence": evidence,
                                    "analysis_report": None,
                                    "target_files": target_files,
                                    "proposed_changes": fix_spec.get("action", "Fix integration issue")
                                },
                                "validation": {
                                    "run_tests": True,
                                    "test_command": "uv run pytest tests/ -v",
                                    "require_pass": True
                                }
                            },
                            "generated_at": datetime.now().timestamp(),
                            "emitted_by": "curiosity_processor_spica_router"
                        }

                        spica_intent_json = json.dumps(spica_intent, indent=2)
                        spica_intent_sha = hashlib.sha256(spica_intent_json.encode()).hexdigest()[:16]
                        spica_intent_path = INTENT_DIR / f"spica_spawn_{spica_intent_sha}_{qid}.json"
                        spica_intent_path.write_text(spica_intent_json)
                        intents_emitted += 1
                        logger.info(f"[spica_spawn] Emitted autonomous fix intent for {qid} (autonomy={autonomy})")

                    _mark_question_processed(qid, doc_intent_sha)
                    continue
                else:
                    logger.warning(f"[integration_fix] No fix generated for {qid}, falling back to D-REAM")
            except Exception as e:
                logger.error(f"[integration_fix] Failed to generate fix for {qid}: {e}")
                # Fall through to D-REAM

        # Route to D-REAM mode based on action_class
        if action_class in ["propose_fix", "explain_and_soft_fallback"]:
            # Direct build - KLoROS provided specific guidance
            logger.info(f"[DIAGNOSTIC] Direct-build mode for {qid} (action={action_class})")
            experiment_result = _spawn_direct_experiment(q)
            experiments_spawned += 1
            logger.info(f"[DIAGNOSTIC] Spawned direct experiment for {qid}, experiments_spawned now={experiments_spawned}")
        else:
            # Tournament mode - open exploration needed
            logger.info(f"Tournament mode for {qid} (action={action_class})")
            experiment_result = _spawn_tournament(q)
            experiments_spawned += 1

        # Also emit intent for orchestrator visibility
        intent = _question_to_intent(q)
        intent["data"]["experiment_result"] = experiment_result

        intent_json = json.dumps(intent, indent=2)
        intent_sha = hashlib.sha256(intent_json.encode()).hexdigest()[:16]
        intent_path = INTENT_DIR / f"curiosity_{intent_sha}_{qid}.json"
        INTENT_DIR.mkdir(parents=True, exist_ok=True)

        try:
            intent_path.write_text(intent_json)
            logger.info(f"Emitted intent for question {qid} (ratio={ratio:.2f}, priority={intent['priority']})")
            intents_emitted += 1

            # Mark as processed
            _mark_question_processed(qid, intent_sha)

        except Exception as e:
            logger.error(f"Failed to emit intent for {qid}: {e}")

    summary = {
        "status": "complete",
        "questions_total": len(questions),
        "intents_emitted": intents_emitted,
        "experiments_spawned": experiments_spawned,
        "skipped_low_value": skipped_low_value,
        "skipped_processed": skipped_processed
    }

    # Structured logging for observability
    logger.info(json.dumps({
        "event": "curiosity_processing_complete",
        "experiments_spawned": experiments_spawned,
        "intents_emitted": intents_emitted,
        "skipped_processed": skipped_processed,
        "skipped_low_value": skipped_low_value,
        "questions_seen": len(questions)
    }))

    return summary


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    result = process_curiosity_feed()
    print(json.dumps(result, indent=2))
