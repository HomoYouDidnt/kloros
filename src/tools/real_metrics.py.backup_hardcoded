#!/usr/bin/env python3
"""
Real Metrics Module for D-REAM Runner Scripts

Shared measurement utilities for WER, latency, VAD, and novelty.
Used by /opt/kloros/tools/* runner scripts.
"""

import json
import time
import sys
import os
import math
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple


def read_json(path: Path) -> dict:
    """Load JSON file."""
    with open(path, 'r') as f:
        return json.load(f)


def load_vad_truth(path_wav: Path) -> Optional[dict]:
    """
    Load VAD ground truth for a wav file.

    Looks for corresponding .vad.json file with format:
    {
        "sample_rate": 16000,
        "segments_ms": [
            {"start": 50, "end": 820},
            {"start": 1040, "end": 1900}
        ]
    }

    Args:
        path_wav: Path to .wav file

    Returns:
        dict with segments_ms, or None if no ground truth
    """
    vad_path = Path(str(path_wav).replace(".wav", ".vad.json"))
    if not vad_path.exists():
        return None
    return read_json(vad_path)


def boundary_error_ms(pred_segments: List[dict], truth: Optional[dict], sr: int = 16000) -> float:
    """
    Calculate VAD boundary error in milliseconds.

    Compares predicted speech segments to ground truth labels.
    Currently measures error on first segment start only (extend as needed).

    Args:
        pred_segments: List of segments from Silero VAD in format:
                      [{'start': sample_idx, 'end': sample_idx}, ...]
        truth: Ground truth dict with segments_ms
        sr: Sample rate for converting samples to milliseconds

    Returns:
        Absolute boundary error in milliseconds
    """
    if not truth or not pred_segments:
        return 100.0  # High error if no data

    if "segments_ms" not in truth or not truth["segments_ms"]:
        return 100.0

    # Convert predicted start from samples to milliseconds
    pred_start_ms = pred_segments[0]['start'] * 1000.0 / sr

    # Get ground truth start
    gt_start_ms = float(truth["segments_ms"][0]["start"])

    # Return absolute error
    return abs(pred_start_ms - gt_start_ms)


def normalize_lang_score(wer: float) -> float:
    """
    Convert Word Error Rate to normalized score (0-1).

    Uses piecewise scaling to reward strong ASR performance:
    - WER ≤ 0.25: Score ≥ 0.85 (excellent, passes 0.78 gate)
    - WER 0.25-0.30: Score 0.70-0.85 (good range)
    - WER ≥ 0.40: Linear degradation (poor)

    Args:
        wer: Word Error Rate (0.0 = perfect, 1.0 = all errors)

    Returns:
        Normalized score (1.0 = perfect, 0.0 = worst)
    """
    if wer >= 0.40:
        # Poor WER: degrade hard
        return max(0.0, 1.0 - wer)
    elif wer <= 0.25:
        # Excellent WER: boost to pass gates
        return 0.85 + (0.25 - wer) * 0.6
    else:
        # Good WER (0.25-0.40): interpolate
        return 0.70 + (0.30 - wer) * 3.0


def calculate_novelty(params: dict, baseline_path: str = "/home/kloros/.kloros/dream_config.json") -> float:
    """
    Calculate parameter novelty vs baseline configuration.

    Measures divergence between experiment parameters and baseline.

    Args:
        params: Current experiment parameters dict
        baseline_path: Path to baseline config JSON

    Returns:
        Novelty score 0-1 (0 = identical, 1 = very different)
    """
    try:
        baseline_data = read_json(Path(baseline_path))
        baseline_params = baseline_data.get("judging", {})

        # Calculate parameter divergence
        all_keys = set(baseline_params.keys()) | set(params.keys())
        divergence = 0.0
        n_keys = 0

        for key in all_keys:
            base_val = baseline_params.get(key, 0)
            curr_val = params.get(key, 0)

            if base_val != 0:
                divergence += abs((curr_val - base_val) / base_val)
                n_keys += 1

        # Normalize to 0-1 range
        if n_keys > 0:
            novelty = min(1.0, divergence / n_keys)
        else:
            novelty = 0.3  # Default if no comparable params

        return round(novelty, 2)

    except Exception as e:
        # Fallback if baseline unavailable
        return 0.30


def measure_asr_latency(audio_path: str, model_size: str = "base", device: str = "cpu") -> int:
    """
    Measure ASR transcription latency in milliseconds.

    Args:
        audio_path: Path to audio file
        model_size: Whisper model size (tiny, base, small, medium)
        device: cpu or cuda

    Returns:
        Latency in milliseconds
    """
    try:
        from faster_whisper import WhisperModel
        import soundfile as sf

        # Determine compute type based on device
        compute_type = "float16" if device == "cuda" else "int8"

        # Load model
        model = WhisperModel(model_size, device=device, compute_type=compute_type)

        # Load audio
        wav, sr = sf.read(audio_path)

        # Measure transcription time
        start = time.time()
        segments, info = model.transcribe(wav, language="en")
        list(segments)  # Force evaluation
        latency_ms = (time.time() - start) * 1000

        return int(latency_ms)

    except Exception as e:
        # Fallback latency if measurement fails
        return 180


def measure_wer_from_eval_set(
    eval_dir: str = "/home/kloros/assets/asr_eval/mini_eval_set",
    backend: str = "vosk",
    output_json: str = "/tmp/wer_measurement.json"
) -> float:
    """
    Measure WER using existing asr_wer.py tool.

    Args:
        eval_dir: Directory with audio files and reference transcripts
        backend: ASR backend (vosk, whisper)
        output_json: Temporary output file

    Returns:
        Overall WER (0.0-1.0)
    """
    try:
        import subprocess

        # Run existing WER evaluation tool
        cmd = [
            "python3", "/home/kloros/tools/audio/asr_wer.py",
            "--backend", backend,
            "--data", eval_dir,
            "--out", output_json
        ]

        subprocess.run(cmd, capture_output=True, timeout=60)

        # Load and parse results
        result = read_json(Path(output_json))
        return result.get("summary", {}).get("overall_wer", 0.25)

    except Exception as e:
        # Fallback WER if measurement fails
        return 0.25


def measure_vad_boundary(
    audio_path: str,
    threshold: float = 0.5,
    sr: int = 16000
) -> Tuple[float, List[dict]]:
    """
    Measure VAD boundary accuracy against ground truth.

    Args:
        audio_path: Path to .wav file (should have matching .vad.json)
        threshold: Silero VAD threshold
        sr: Sample rate

    Returns:
        Tuple of (boundary_error_ms, predicted_segments)
    """
    try:
        import torch
        import soundfile as sf

        # Load audio
        wav, audio_sr = sf.read(audio_path)

        # Resample to 16kHz if needed
        if audio_sr != sr:
            import librosa
            wav = librosa.resample(wav, orig_sr=audio_sr, target_sr=sr)

        # Load Silero VAD
        vad_model, utils = torch.hub.load(
            repo_or_dir="snakers4/silero-vad",
            model="silero_vad",
            force_reload=False,
            onnx=False
        )
        get_speech_timestamps = utils[0]

        # Get speech timestamps
        wav_tensor = torch.from_numpy(wav.astype(np.float32))
        speech_timestamps = get_speech_timestamps(
            wav_tensor,
            vad_model,
            sampling_rate=sr,
            threshold=threshold
        )

        # Load ground truth
        truth = load_vad_truth(Path(audio_path))

        # Calculate boundary error
        error_ms = boundary_error_ms(speech_timestamps, truth, sr)

        return error_ms, speech_timestamps

    except Exception as e:
        # Fallback if VAD measurement fails
        return 60.0, []


# Convenience function for runner scripts
def get_real_metrics(
    eval_audio_path: str = "/home/kloros/assets/asr_eval/mini_eval_set/sample1.wav",
    params: dict = None,
    device: str = None,
    compute_type: str = None,
    model_size: str = None
) -> dict:
    """
    Get all real metrics for a D-REAM evaluation run.

    Args:
        eval_audio_path: Path to evaluation audio file
        params: Experiment parameters dict
        device: Device to use (cpu, cuda) - from params if not specified
        compute_type: Compute type (int8, float16) - from params if not specified
        model_size: Model size (tiny, base, small, medium) - from params if not specified

    Returns:
        Dict with wer, latency_ms, vad_boundary_ms, novelty, score
    """
    if params is None:
        params = {}

    # Extract device/compute settings from params if not explicitly provided
    if device is None:
        device = params.get("device", "cpu")
    if compute_type is None:
        compute_type = params.get("compute_type", "int8")
    if model_size is None:
        model_size = params.get("model_size", "base")

    # Measure WER
    wer = measure_wer_from_eval_set()

    # Measure latency with specified device/compute
    latency_ms = measure_asr_latency(
        eval_audio_path,
        model_size=model_size,
        device=device
    )

    # Measure VAD boundary
    vad_boundary_ms, _ = measure_vad_boundary(eval_audio_path)

    # Calculate novelty
    novelty = calculate_novelty(params)

    # Calculate score from WER
    score = normalize_lang_score(wer)

    return {
        "wer": round(wer, 3),
        "latency_ms": latency_ms,
        "vad_boundary_ms": int(vad_boundary_ms),
        "novelty": novelty,
        "score": round(score, 2)
    }


if __name__ == "__main__":
    # Test the module
    print("Testing real_metrics module...")

    # Test with defaults
    metrics = get_real_metrics()

    print("\nMeasured Metrics:")
    print(f"  WER: {metrics['wer']:.3f}")
    print(f"  Latency: {metrics['latency_ms']}ms")
    print(f"  VAD Boundary Error: {metrics['vad_boundary_ms']}ms")
    print(f"  Novelty: {metrics['novelty']:.2f}")
    print(f"  Score: {metrics['score']:.2f}")
