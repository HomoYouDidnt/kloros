#!/usr/bin/env python3
"""
Exploration Scanner - Proactive Curiosity Question Generator

Generates "what if" questions based on:
- Hardware discoveries
- System observations
- Optimization opportunities
- Creative possibilities

Unlike gap scanner (finds what's missing) or duplication scanner (finds redundancy),
this scanner finds OPPORTUNITIES for improvement and exploration.

Governance:
- Tool-Integrity: Self-contained, generates questions only, no destructive ops
- D-REAM-Allowed-Stack: Analysis only, no system modifications
- Creativity-First: Generates novel exploration ideas, not just fixes
"""

import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from datetime import datetime

from src.cognition.mind.cognition.hardware_scanner import HardwareCapabilities, scan_hardware

logger = logging.getLogger(__name__)

EXPLORATION_TRACKER = Path.home() / '.kloros' / 'curiosity' / 'exploration_generated.json'


@dataclass
class ExplorationQuestion:
    """A proactive exploration question generated by curiosity."""
    id: str
    hypothesis: str
    question: str
    category: str  # hardware, optimization, architecture, creative
    evidence: List[str]
    value_estimate: float  # 0.0 - 1.0
    cost: float  # 0.0 - 1.0 (effort required)
    action_class: str  # investigate, experiment, optimize, explore
    autonomy: int = 2  # Autonomy level required to pursue
    created_at: str = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now().isoformat()

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "hypothesis": self.hypothesis,
            "question": self.question,
            "evidence": self.evidence,
            "evidence_hash": None,
            "action_class": self.action_class,
            "autonomy": self.autonomy,
            "value_estimate": self.value_estimate,
            "cost": self.cost,
            "status": "ready",
            "created_at": self.created_at,
            "capability_key": f"exploration.{self.category}.{self.id}"
        }


class ExplorationScanner:
    """Generates proactive exploration questions from system observations."""

    def __init__(self):
        self.questions: List[ExplorationQuestion] = []
        self.generated_ids: Set[str] = self._load_generated_ids()

    def _load_generated_ids(self) -> Set[str]:
        """Load set of previously generated question IDs."""
        if not EXPLORATION_TRACKER.exists():
            return set()
        try:
            with open(EXPLORATION_TRACKER, 'r') as f:
                data = json.load(f)
                return set(data.get('generated_ids', []))
        except Exception as e:
            logger.warning(f"[exploration_scanner] Failed to load tracker: {e}")
            return set()

    def _save_generated_ids(self) -> None:
        """Save generated question IDs to tracker file."""
        try:
            EXPLORATION_TRACKER.parent.mkdir(parents=True, exist_ok=True)
            with open(EXPLORATION_TRACKER, 'w') as f:
                json.dump({
                    'generated_ids': list(self.generated_ids),
                    'last_updated': datetime.now().isoformat()
                }, f, indent=2)
        except Exception as e:
            logger.warning(f"[exploration_scanner] Failed to save tracker: {e}")

    def _should_generate(self, question_id: str) -> bool:
        """Check if question should be generated (not already generated)."""
        return question_id not in self.generated_ids

    def scan(self) -> List[ExplorationQuestion]:
        """
        Generate exploration questions across all categories.

        Only generates questions that haven't been generated before.
        Tracks generated IDs to prevent duplicate question spam.

        Returns:
            List of ExplorationQuestion objects (only new questions)
        """
        logger.info(f"[exploration_scanner] Starting proactive exploration scan (already generated: {len(self.generated_ids)})")

        # Scan hardware for optimization opportunities
        try:
            hardware = scan_hardware()
            self._explore_hardware(hardware)
        except Exception as e:
            logger.warning(f"[exploration_scanner] Hardware exploration failed: {e}")

        # Future: Add more exploration categories
        # self._explore_architecture()
        # self._explore_performance()
        # self._explore_chem_bus()

        # Save newly generated question IDs
        if self.questions:
            for q in self.questions:
                self.generated_ids.add(q.id)
            self._save_generated_ids()

        logger.info(f"[exploration_scanner] Generated {len(self.questions)} NEW exploration questions")
        return self.questions

    def _explore_hardware(self, hw: HardwareCapabilities) -> None:
        """Generate hardware-based exploration questions."""

        # Multi-GPU opportunities
        if len(hw.gpus) > 1 and self._should_generate("multi_gpu_parallelization"):
            self.questions.append(ExplorationQuestion(
                id="multi_gpu_parallelization",
                hypothesis="MULTI_GPU_PARALLELIZATION_OPPORTUNITY",
                question=f"I have {len(hw.gpus)} GPUs ({hw.total_vram_mb}MB total VRAM). "
                        f"Could I parallelize inference across them? What workloads benefit from multi-GPU?",
                category="hardware",
                evidence=[
                    f"GPU count: {len(hw.gpus)}",
                    f"Total VRAM: {hw.total_vram_mb}MB",
                    f"GPUs: {', '.join(gpu.name for gpu in hw.gpus)}"
                ],
                value_estimate=0.85,
                cost=0.6,
                action_class="investigate"
            ))

        # Tensor core opportunities
        tensor_core_gpus = [gpu for gpu in hw.gpus if gpu.tensor_cores]
        if tensor_core_gpus:
            for gpu in tensor_core_gpus:
                q_id = f"tensor_cores_gpu{gpu.index}"
                if self._should_generate(q_id):
                    self.questions.append(ExplorationQuestion(
                        id=q_id,
                        hypothesis=f"TENSOR_CORE_ACCELERATION_GPU{gpu.index}",
                        question=f"GPU {gpu.index} ({gpu.name}) has tensor cores. "
                                f"Can I leverage them for faster inference? What models support tensor core acceleration?",
                        category="hardware",
                        evidence=[
                            f"GPU {gpu.index}: {gpu.name}",
                            "Tensor cores: available",
                            f"VRAM: {gpu.memory_total_mb}MB"
                        ],
                        value_estimate=0.9,
                        cost=0.5,
                        action_class="experiment"
                    ))

        # PCIe bottleneck detection
        for gpu in hw.gpus:
            if gpu.pcie_width and gpu.pcie_width < 16:
                q_id = f"pcie_bottleneck_gpu{gpu.index}"
                if self._should_generate(q_id):
                    self.questions.append(ExplorationQuestion(
                        id=q_id,
                        hypothesis=f"PCIE_BOTTLENECK_GPU{gpu.index}",
                        question=f"GPU {gpu.index} is running at PCIe Gen{gpu.pcie_gen} x{gpu.pcie_width} "
                                f"(should be x16). This is a bottleneck! Why? Can I fix the PCIe lane configuration?",
                        category="hardware",
                        evidence=[
                            f"GPU {gpu.index}: {gpu.name}",
                            f"PCIe: Gen{gpu.pcie_gen} x{gpu.pcie_width}",
                            "Expected: x16 for full bandwidth"
                        ],
                        value_estimate=0.95,
                        cost=0.3,  # Low cost to investigate
                        action_class="investigate"
                    ))

        # GPU swap suggestion - if better GPU is in worse slot
        if len(hw.gpus) == 2:
            gpu0, gpu1 = hw.gpus[0], hw.gpus[1]
            # Check if GPU 0 has tensor cores but worse PCIe, and GPU 1 has better PCIe
            if (gpu0.tensor_cores and not gpu1.tensor_cores and
                gpu0.pcie_width and gpu1.pcie_width and
                gpu0.pcie_width < gpu1.pcie_width and
                self._should_generate("gpu_physical_swap_suggestion")):
                self.questions.append(ExplorationQuestion(
                    id="gpu_physical_swap_suggestion",
                    hypothesis="GPU_PHYSICAL_SWAP_OPTIMIZATION",
                    question=f"GPU {gpu0.index} ({gpu0.name}) has tensor cores but is in the slower "
                            f"PCIe x{gpu0.pcie_width} slot, while GPU {gpu1.index} is in the faster x{gpu1.pcie_width} slot. "
                            f"Should I suggest swapping the physical GPU positions to give the better GPU full bandwidth?",
                    category="creative",
                    evidence=[
                        f"GPU {gpu0.index}: Tensor cores=True, PCIe x{gpu0.pcie_width}",
                        f"GPU {gpu1.index}: Tensor cores=False, PCIe x{gpu1.pcie_width}",
                        "Better GPU in worse slot - suboptimal configuration"
                    ],
                    value_estimate=0.98,  # Very high value - hardware reconfiguration
                    cost=0.2,  # Low cost - just a suggestion to user
                    action_class="propose_fix"
                ))

        # VRAM optimization opportunities
        if hw.total_vram_mb > 16000 and self._should_generate("vram_optimization_strategy"):  # More than 16GB total
            self.questions.append(ExplorationQuestion(
                id="vram_optimization_strategy",
                hypothesis="VRAM_OPTIMIZATION_MULTI_GPU",
                question=f"I have {hw.total_vram_mb}MB VRAM across {len(hw.gpus)} GPUs. "
                        f"Could I load multiple models simultaneously? What's my optimal VRAM allocation strategy?",
                category="optimization",
                evidence=[
                    f"Total VRAM: {hw.total_vram_mb}MB",
                    f"GPUs: {len(hw.gpus)}",
                    "Sufficient for multiple large models"
                ],
                value_estimate=0.8,
                cost=0.4,
                action_class="experiment"
            ))

        # GPU selection strategy
        if len(hw.gpus) > 1:
            # Find GPU with best bandwidth
            best_bandwidth_gpu = max(
                hw.gpus,
                key=lambda g: (g.pcie_width or 0) * (g.pcie_gen or 0)
            )
            # Find GPU with most VRAM
            most_vram_gpu = max(
                hw.gpus,
                key=lambda g: g.memory_total_mb or 0
            )

            if best_bandwidth_gpu.index != most_vram_gpu.index and self._should_generate("gpu_selection_strategy"):
                self.questions.append(ExplorationQuestion(
                    id="gpu_selection_strategy",
                    hypothesis="OPTIMAL_GPU_SELECTION_STRATEGY",
                    question=f"GPU {best_bandwidth_gpu.index} has best PCIe bandwidth (x{best_bandwidth_gpu.pcie_width}), "
                            f"but GPU {most_vram_gpu.index} has most VRAM ({most_vram_gpu.memory_total_mb}MB). "
                            f"Which should I use for primary inference? Should I split workloads?",
                    category="optimization",
                    evidence=[
                        f"GPU {best_bandwidth_gpu.index}: Best bandwidth (Gen{best_bandwidth_gpu.pcie_gen} x{best_bandwidth_gpu.pcie_width})",
                        f"GPU {most_vram_gpu.index}: Most VRAM ({most_vram_gpu.memory_total_mb}MB)",
                        "Optimization tradeoff: bandwidth vs capacity"
                    ],
                    value_estimate=0.85,
                    cost=0.5,
                    action_class="investigate"
                ))

        # Memory constraints vs available resources
        if hw.ram_available_mb < 8192 and self._should_generate("memory_pressure_investigation"):  # Less than 8GB available RAM
            self.questions.append(ExplorationQuestion(
                id="memory_pressure_investigation",
                hypothesis="MEMORY_PRESSURE_OPTIMIZATION",
                question=f"Only {hw.ram_available_mb}MB RAM available (out of {hw.ram_total_mb}MB). "
                        f"What's consuming memory? Can I optimize memory usage or offload to GPUs?",
                category="optimization",
                evidence=[
                    f"RAM total: {hw.ram_total_mb}MB",
                    f"RAM available: {hw.ram_available_mb}MB",
                    "Low memory availability"
                ],
                value_estimate=0.9,
                cost=0.3,
                action_class="investigate"
            ))

        # Creative: Inter-GPU communication
        if len(hw.gpus) > 1 and self._should_generate("inter_gpu_communication"):
            self.questions.append(ExplorationQuestion(
                id="inter_gpu_communication",
                hypothesis="INTER_GPU_MESSAGING_VIA_CHEM_BUS",
                question=f"Could I extend chem_bus to route messages between GPU {hw.gpus[0].index} and "
                        f"GPU {hw.gpus[1].index}? Enable GPU-to-GPU work distribution?",
                category="creative",
                evidence=[
                    f"{len(hw.gpus)} GPUs available",
                    "chem_bus exists for message routing",
                    "Potential for distributed GPU work"
                ],
                value_estimate=0.75,
                cost=0.7,  # Higher cost, more experimental
                action_class="explore"
            ))

        # Creative: Dynamic GPU allocation
        if hw.total_vram_mb > 16000 and self._should_generate("dynamic_gpu_allocation"):
            self.questions.append(ExplorationQuestion(
                id="dynamic_gpu_allocation",
                hypothesis="DYNAMIC_VRAM_ALLOCATION",
                question=f"With {hw.total_vram_mb}MB total VRAM, could I dynamically allocate models to GPUs "
                        f"based on current workload? Implement smart VRAM management?",
                category="creative",
                evidence=[
                    f"Total VRAM: {hw.total_vram_mb}MB",
                    "Multiple GPUs for load balancing",
                    "Opportunity for intelligent allocation"
                ],
                value_estimate=0.8,
                cost=0.8,
                action_class="explore"
            ))


def generate_exploration_questions() -> List[Dict[str, Any]]:
    """
    Main entry point for exploration question generation.

    Returns:
        List of questions as dictionaries (ready for curiosity feed)
    """
    scanner = ExplorationScanner()
    questions = scanner.scan()
    return [q.to_dict() for q in questions]


if __name__ == "__main__":
    # Test exploration scanner
    logging.basicConfig(level=logging.INFO)
    questions = generate_exploration_questions()

    import json
    print(json.dumps(questions, indent=2))
