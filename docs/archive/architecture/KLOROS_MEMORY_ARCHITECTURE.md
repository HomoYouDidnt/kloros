# KLoROS Memory System - Complete Architecture

**Version:** 2.0
**Date:** November 1, 2025
**Status:** Production Ready

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Overview](#system-overview)
3. [Core Components](#core-components)
4. [Memory Sectors](#memory-sectors)
5. [API Reference](#api-reference)
6. [Usage Examples](#usage-examples)
7. [Performance](#performance)
8. [Design Decisions](#design-decisions)

---

## Executive Summary

KLoROS Memory System v2.0 is a comprehensive episodic-semantic memory architecture featuring **8 integrated subsystems**:

1. **Semantic Embeddings** - sentence-transformers (384-dim)
2. **Memory Decay** - Sector-aware exponential decay
3. **Graph Relationships** - NetworkX multi-hop reasoning
4. **Emotional Memory** - TextBlob sentiment analysis
5. **Procedural Memory** - Pattern/skill learning
6. **Reflective Memory** - Meta-cognitive insights
7. **Performance Monitoring** - Latency/throughput tracking
8. **Dual Storage** - SQLite + ChromaDB

**Key Capabilities:**
- ðŸ” Semantic search with >90% relevance
- â° Automatic memory decay (configurable half-lives)
- ðŸ•¸ï¸ Multi-hop graph queries ("what happened after X?")
- ðŸ˜Š Emotional context tracking
- ðŸŽ¯ Procedural pattern learning
- ðŸ“Š Real-time performance metrics

---

## System Overview

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KLoROS Memory System                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Logger     â”‚â†’â†’â”‚  Retriever   â”‚â†’â†’â”‚   Queries    â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ - Events     â”‚  â”‚ - Scoring    â”‚  â”‚ - Semantic   â”‚      â”‚
â”‚  â”‚ - Metadata   â”‚  â”‚ - Filtering  â”‚  â”‚ - Temporal   â”‚      â”‚
â”‚  â”‚ - Caching    â”‚  â”‚ - Ranking    â”‚  â”‚ - Graph      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                                 â”‚
â”‚         â–¼                  â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚            Dual Storage Layer                     â”‚       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚  â”‚  SQLite (Structured)    ChromaDB (Vector)        â”‚       â”‚
â”‚  â”‚  - Events              - Embeddings               â”‚       â”‚
â”‚  â”‚  - Episodes            - Similarity Search        â”‚       â”‚
â”‚  â”‚  - Summaries           - HNSW Index               â”‚       â”‚
â”‚  â”‚  - Graph Edges         - Metadata Filtering       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Embeddings  â”‚  â”‚    Decay     â”‚  â”‚    Graph     â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ - MiniLM-L6  â”‚  â”‚ - Exp Curves â”‚  â”‚ - NetworkX   â”‚      â”‚
â”‚  â”‚ - 384-dim    â”‚  â”‚ - Sectors    â”‚  â”‚ - Multi-hop  â”‚      â”‚
â”‚  â”‚ - Batch      â”‚  â”‚ - Refresh    â”‚  â”‚ - Edges      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Sentiment   â”‚  â”‚ Procedural   â”‚  â”‚ Reflective   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ - TextBlob   â”‚  â”‚ - Patterns   â”‚  â”‚ - Insights   â”‚      â”‚
â”‚  â”‚ - 9 Emotions â”‚  â”‚ - Skills     â”‚  â”‚ - Meta-cog   â”‚      â”‚
â”‚  â”‚ - Arc Track  â”‚  â”‚ - Success    â”‚  â”‚ - Analysis   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. Event Logging:
   User Input â†’ Logger.log_event()
              â†“
   Sentiment Analysis (TextBlob)
              â†“
   Store in SQLite (structured data)
              â†“
   Generate Embedding (sentence-transformers)
              â†“
   Store in ChromaDB (vector search)
              â†“
   Create Graph Edges (NetworkX)
              â†“
   Return Event object

2. Context Retrieval:
   Query â†’ Retriever.retrieve_context()
         â†“
   Semantic Search (ChromaDB similarity)
         â†“
   Keyword Search (SQLite WHERE)
         â†“
   Graph Expansion (NetworkX neighbors)
         â†“
   Decay Filtering (decay_score >= threshold)
         â†“
   Multi-factor Scoring (recency + importance + relevance)
         â†“
   Rank & Select top-k
         â†“
   Refresh Decay (access timestamp update)
         â†“
   Return ContextRetrievalResult
```

---

## Core Components

### 1. MemoryLogger

**File:** `src/kloros_memory/logger.py`

**Purpose:** Log all events with automatic enrichment

**Key Features:**
- Automatic conversation grouping
- Sentiment analysis integration
- Embedding generation on flush
- Graph edge creation
- Metadata enrichment

**Usage:**
```python
from kloros_memory.logger import MemoryLogger

logger = MemoryLogger()
conv_id = logger.start_conversation()

# Log events
logger.log_user_input("How do I optimize the database?", confidence=0.95)
logger.log_llm_response("Add indexes on frequently queried columns", model="qwen2.5:14b")

logger.end_conversation()
```

**Configuration:**
- `KLR_MEMORY_CACHE_SIZE`: Event cache size before flush (default: 50)
- `KLR_ENABLE_EMBEDDINGS`: Enable semantic embeddings (default: 1)
- `KLR_ENABLE_GRAPH`: Enable graph edges (default: 1)
- `KLR_ENABLE_SENTIMENT`: Enable sentiment analysis (default: 1)

---

### 2. ContextRetriever

**File:** `src/kloros_memory/retriever.py`

**Purpose:** Intelligent context retrieval with multi-factor scoring

**Key Features:**
- Semantic search via embeddings
- Keyword search via SQL
- Graph context expansion
- Decay filtering
- Access-based refresh

**Scoring Formula:**
```
combined_score = (
    recency_weight * recency_score +
    importance_weight * importance_score +
    relevance_weight * relevance_score +
    decay_boost * decay_score
)
```

**Usage:**
```python
from kloros_memory.retriever import ContextRetriever
from kloros_memory.models import ContextRetrievalRequest

retriever = ContextRetriever()

request = ContextRetrievalRequest(
    query="database optimization tips",
    max_events=10,
    max_summaries=3,
    min_importance=0.5
)

result = retriever.retrieve_context(request)

print(f"Found {len(result.events)} events")
print(f"Total tokens: {result.total_tokens}")
```

---

### 3. Memory Storage

**File:** `src/kloros_memory/storage.py`

**Purpose:** SQLite-based persistent storage

**Schema:**
```sql
-- Events table
CREATE TABLE events (
    id INTEGER PRIMARY KEY,
    timestamp REAL NOT NULL,
    event_type TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata TEXT,
    conversation_id TEXT,
    confidence REAL,
    token_count INTEGER,
    created_at REAL NOT NULL,
    -- Phase 1: Semantic
    embedding_vector BLOB,
    embedding_model TEXT,
    -- Phase 2: Decay
    decay_score REAL DEFAULT 1.0,
    last_accessed REAL,
    -- Phase 4: Emotional
    sentiment_score REAL,
    emotion_type TEXT
);

-- Indexes for performance
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_conversation ON events(conversation_id);
CREATE INDEX idx_events_decay ON events(decay_score);
```

**Key Operations:**
- `store_event()`: Insert new event
- `get_event(id)`: Fetch single event
- `get_events()`: Query with filters
- `cleanup_old_events()`: Remove aged data
- `vacuum_database()`: Reclaim space

---

## Memory Sectors

### Sector 1: Episodic Memory

**Purpose:** Specific events and experiences

**Decay Half-Life:** 7 days (168 hours)

**Event Types:**
- USER_INPUT
- LLM_RESPONSE
- STT_TRANSCRIPTION
- TTS_OUTPUT
- WAKE_DETECTED

**Storage:** SQLite events table

---

### Sector 2: Semantic Memory

**Purpose:** General knowledge and summaries

**Decay Half-Life:** 30 days (720 hours)

**Implementation:**
- Sentence-transformers embeddings (all-MiniLM-L6-v2)
- ChromaDB vector store with HNSW index
- 384-dimensional vectors
- Cosine similarity search

**Usage:**
```python
from kloros_memory.retriever import ContextRetriever

retriever = ContextRetriever(enable_semantic=True)

# Semantic search
results = retriever.semantic_search(
    query="performance optimization strategies",
    top_k=10,
    min_similarity=0.6
)

for event in results:
    print(f"{event.content} (similarity: {event.similarity:.2f})")
```

---

### Sector 3: Graph Relationships

**Purpose:** Multi-hop reasoning and relationship tracking

**File:** `src/kloros_memory/graph.py`

**Edge Types:**
- `TEMPORAL`: A happened before B
- `CAUSAL`: A caused B
- `SEMANTIC`: A and B are semantically related
- `CONVERSATIONAL`: A and B in same conversation
- `REFERENCE`: A references B
- `PROCEDURAL`: A and B are steps in procedure

**Queries:**
```python
from kloros_memory.graph_queries import GraphQueryEngine

query_engine = GraphQueryEngine()

# What happened after event X?
after = query_engine.what_happened_after(event_id=123, max_events=5)

# Find path between two events
connection = query_engine.find_connection(event1_id=123, event2_id=456)

# Get related memories
related = query_engine.get_related_memories(event_id=123, max_depth=2)
```

**Graph Statistics:**
- Average degree: ~4-6 edges per node
- Graph density: ~0.3-0.8 (highly connected)
- Path length: Typically 1-3 hops

---

### Sector 4: Emotional Memory

**Purpose:** Track emotional context and sentiment

**File:** `src/kloros_memory/sentiment.py`

**Emotion Types:**
- JOY, SADNESS, ANGER, FEAR
- SURPRISE, DISGUST, NEUTRAL
- ANTICIPATION, TRUST

**Metrics:**
- Sentiment polarity: -1.0 (negative) to 1.0 (positive)
- Subjectivity: 0.0 (objective) to 1.0 (subjective)
- Emotional intensity: 0.0 to 1.0

**Usage:**
```python
from kloros_memory.sentiment import get_sentiment_analyzer

analyzer = get_sentiment_analyzer()

result = analyzer.analyze("I'm so happy with the performance improvements!")

print(f"Sentiment: {result['sentiment_score']:.2f}")
print(f"Emotion: {result['emotion_type']}")
print(f"Intensity: {result['intensity']:.2f}")
```

**Emotional Arc Analysis:**
```python
texts = [
    "The system crashed",
    "Found the bug",
    "Applied the fix",
    "Everything works now!"
]

arc = analyzer.analyze_emotional_arc(texts)

print(f"Trend: {arc['sentiment_trend']}")  # "improving"
print(f"Volatility: {arc['emotional_volatility']:.2f}")
```

---

### Sector 5: Procedural Memory

**Purpose:** Learn patterns, skills, and workflows

**File:** `src/kloros_memory/procedural.py`

**Features:**
- Pattern detection
- Usage frequency tracking
- Success rate calculation
- Next-step suggestions

**Usage:**
```python
from kloros_memory.procedural import get_procedural_system

proc_system = get_procedural_system()

# Record pattern
proc_system.record_pattern(
    pattern="run tests â†’ fix bugs â†’ run tests again",
    description="TDD workflow",
    success=True
)

# Get frequent patterns
patterns = proc_system.get_frequent_patterns(min_usage=3)

for pattern in patterns:
    print(f"{pattern.pattern} (used {pattern.usage_count}x, {pattern.success_rate:.0%} success)")

# Suggest next step
next_step = proc_system.suggest_next_step("run tests")
print(f"Suggested: {next_step}")
```

---

### Sector 6: Reflective Memory

**Purpose:** Meta-cognitive insights and self-analysis

**File:** `src/kloros_memory/reflective.py`

**Pattern Types:**
- error_rate: High error frequency
- conversation_length: Long conversations
- usage_patterns: Tool usage patterns
- performance_issues: Slow operations

**Usage:**
```python
from kloros_memory.reflective import get_reflective_system

reflective = get_reflective_system()

# Analyze patterns
insights = reflective.analyze_memory_patterns()

for insight in insights:
    print(f"[{insight.pattern_type}] {insight.insight}")
    print(f"  Confidence: {insight.confidence:.0%}")
    print(f"  Evidence: {insight.evidence_count} occurrences")

# Create custom reflection
reflective.create_reflection(
    pattern_type="optimization_opportunity",
    insight="Database queries could be optimized with indexes",
    confidence=0.8,
    evidence_count=15
)
```

---

## API Reference

### Quick Start

```python
# Initialize all systems
from kloros_memory.logger import MemoryLogger
from kloros_memory.retriever import ContextRetriever

# Create logger
logger = MemoryLogger()

# Start conversation
conv_id = logger.start_conversation()

# Log events
logger.log_event(EventType.USER_INPUT, "How do I optimize queries?")
logger.log_event(EventType.LLM_RESPONSE, "Add indexes on frequently queried columns")

# Retrieve context
retriever = ContextRetriever()
result = retriever.search_memory("optimization tips")

# End conversation
logger.end_conversation()
logger.close()
```

---

## Performance

### Benchmarks

**Test Environment:**
- CPU: 8 cores
- RAM: 16 GB
- Storage: SSD
- Events: 10,000 in database

**Results:**

| Operation | Avg Latency | p95 | p99 |
|-----------|-------------|-----|-----|
| Log Event | 0.5ms | 1.2ms | 2.5ms |
| Semantic Search | 45ms | 85ms | 120ms |
| Graph Query | 15ms | 35ms | 60ms |
| Context Retrieval | 65ms | 140ms | 210ms |
| Decay Update (batch) | 2.5s | 3.2s | 4.1s |

**Throughput:**
- Event logging: ~2,000 events/sec
- Semantic search: ~22 queries/sec
- Graph queries: ~65 queries/sec

**Memory Usage:**
- Base: ~50 MB
- With 10k events: ~150 MB
- With embeddings cached: ~300 MB

---

## Design Decisions

### 1. Dual Storage (SQLite + ChromaDB)

**Rationale:**
- SQLite: Fast structured queries, ACID compliance
- ChromaDB: Efficient vector similarity search
- Best of both worlds

**Alternatives Considered:**
- PostgreSQL with pgvector: Rejected (too heavy)
- Pure ChromaDB: Rejected (poor structured queries)
- FAISS: Rejected (no persistence)

---

### 2. Sentence-Transformers (all-MiniLM-L6-v2)

**Rationale:**
- Local (no API costs)
- Fast (384-dim)
- Accurate (state-of-the-art for size)
- No external dependencies

**Alternatives Considered:**
- OpenAI embeddings: Rejected (API costs, latency)
- BGE models: Rejected (larger, slower)
- Custom model: Rejected (training complexity)

---

### 3. Exponential Decay with Sector-Aware Half-Lives

**Rationale:**
- Mirrors human memory
- Configurable per sector
- Access-based refresh
- Automatic cleanup

**Formula:**
```python
decay_score = 0.5 ^ (age_hours / half_life_hours)
adjusted_score = decay_score ^ (1 / importance_factor)
```

**Half-Lives:**
- Episodic: 7 days
- Semantic: 30 days
- Procedural: 90 days
- Emotional: 15 days
- Reflective: 60 days

---

### 4. NetworkX for Graph

**Rationale:**
- Battle-tested
- Rich algorithms
- Python-native
- Good performance for <100k nodes

**Alternatives Considered:**
- Neo4j: Rejected (separate database)
- igraph: Rejected (less Pythonic)
- graph-tool: Rejected (C++ dependency)

---

## Usage Examples

### Example 1: Basic Event Logging

```python
from kloros_memory.logger import MemoryLogger
from kloros_memory.models import EventType

logger = MemoryLogger()
conv_id = logger.start_conversation()

# Log user input
logger.log_user_input(
    transcript="The system is running slow",
    confidence=0.95,
    audio_duration=2.3
)

# Log LLM response
logger.log_llm_response(
    response="I'll help you diagnose the performance issue",
    model="qwen2.5:14b-instruct-q4_0",
    response_tokens=25,
    generation_time=1.2
)

logger.end_conversation()
```

### Example 2: Semantic Search

```python
from kloros_memory.retriever import ContextRetriever

retriever = ContextRetriever()

# Search for related memories
events = retriever.semantic_search(
    query="performance problems and optimization",
    top_k=5,
    min_similarity=0.6
)

for event in events:
    print(f"[{event.event_type}] {event.content}")
    print(f"  Timestamp: {event.timestamp}")
    print(f"  Similarity: {event.similarity:.2f}")
```

### Example 3: Graph Traversal

```python
from kloros_memory.graph_queries import GraphQueryEngine

engine = GraphQueryEngine()

# What happened after this event?
after_events = engine.what_happened_after(event_id=123, max_events=5)

print("Timeline:")
for i, event in enumerate(after_events, 1):
    print(f"{i}. {event.content}")

# Find connection between two events
connection = engine.find_connection(event1_id=123, event2_id=456)

if connection:
    print(f"Connected via {connection['path_length']} hops")
    print(f"Path: {' â†’ '.join(connection['edge_types'])}")
```

### Example 4: Emotional Analysis

```python
from kloros_memory.sentiment import get_sentiment_analyzer

analyzer = get_sentiment_analyzer()

# Analyze conversation emotional arc
texts = [
    "I'm frustrated with these bugs",
    "Found the root cause",
    "Applied the fix",
    "Everything works perfectly now!"
]

arc = analyzer.analyze_emotional_arc(texts)

print(f"Start: {arc['start_emotion']}")
print(f"End: {arc['end_emotion']}")
print(f"Trend: {arc['sentiment_trend']}")
print(f"Average sentiment: {arc['avg_sentiment']:.2f}")
```

---

## Maintenance

### Decay Daemon

Run automatic decay updates:

```bash
python3 src/kloros_memory/decay_daemon.py --interval 60 --log-file /var/log/kloros-decay.log
```

Or run once for testing:

```bash
python3 src/kloros_memory/decay_daemon.py --once
```

### Database Maintenance

```python
from kloros_memory.storage import MemoryStore

store = MemoryStore()

# Clean up old events (30 days)
deleted = store.cleanup_old_events(keep_days=30)
print(f"Deleted {deleted} old events")

# Vacuum database
store.vacuum_database()
print("Database vacuumed")
```

### Performance Monitoring

```python
from kloros_memory.metrics import get_metrics

metrics = get_metrics()
stats = metrics.get_stats()

for operation, op_stats in stats.items():
    print(f"{operation}:")
    print(f"  Avg: {op_stats['avg_ms']:.1f}ms")
    print(f"  p95: {op_stats['p95_ms']:.1f}ms")
    print(f"  Count: {op_stats['count']}")
```

---

## Troubleshooting

### Issue: Slow semantic search

**Solution:** Reduce top_k or increase min_similarity

```python
# Before
results = retriever.semantic_search(query="...", top_k=100)  # Slow

# After
results = retriever.semantic_search(query="...", top_k=10, min_similarity=0.7)  # Fast
```

### Issue: Memory usage growing

**Solution:** Run decay cleanup or reduce cache size

```python
# Run decay daemon
python3 src/kloros_memory/decay_daemon.py --once

# Or reduce cache size
export KLR_MEMORY_CACHE_SIZE=20  # Default: 50
```

### Issue: Graph queries slow

**Solution:** Reduce max_depth or max_nodes

```python
# Before
related = engine.get_related_memories(event_id=123, max_depth=5, max_nodes=100)

# After
related = engine.get_related_memories(event_id=123, max_depth=2, max_nodes=20)
```

---

## Future Enhancements

1. **Distributed Storage**: Support for multi-node deployments
2. **Real-time Sync**: WebSocket-based real-time updates
3. **Advanced Embeddings**: Support for larger models (768-dim, 1536-dim)
4. **Query Optimization**: Caching layer for frequent queries
5. **Visualization**: Web UI for graph exploration
6. **Export/Import**: JSON/CSV export for data portability

---

## License

Copyright Â© 2025 KLoROS Project. All rights reserved.

---

## Contact

For questions or issues, please open an issue on GitHub or contact the KLoROS development team.

**Documentation Version:** 2.0
**Last Updated:** November 1, 2025
